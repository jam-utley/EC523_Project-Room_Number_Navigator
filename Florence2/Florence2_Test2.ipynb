{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KtkEy0TqOLU",
        "outputId": "1e6346dc-e2aa-4cc1-ca88-cb0d1c42d3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/Florence-2-base were not used when initializing Florence2ForConditionalGeneration: ['image_pos_embed.column_embeddings.weight', 'image_pos_embed.row_embeddings.weight', 'image_proj_norm.bias', 'image_proj_norm.weight', 'image_projection', 'language_model.final_logits_bias', 'language_model.model.decoder.embed_positions.weight', 'language_model.model.decoder.layernorm_embedding.bias', 'language_model.model.decoder.layernorm_embedding.weight', 'language_model.model.decoder.layers.0.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.0.fc1.bias', 'language_model.model.decoder.layers.0.fc1.weight', 'language_model.model.decoder.layers.0.fc2.bias', 'language_model.model.decoder.layers.0.fc2.weight', 'language_model.model.decoder.layers.0.final_layer_norm.bias', 'language_model.model.decoder.layers.0.final_layer_norm.weight', 'language_model.model.decoder.layers.0.self_attn.k_proj.bias', 'language_model.model.decoder.layers.0.self_attn.k_proj.weight', 'language_model.model.decoder.layers.0.self_attn.out_proj.bias', 'language_model.model.decoder.layers.0.self_attn.out_proj.weight', 'language_model.model.decoder.layers.0.self_attn.q_proj.bias', 'language_model.model.decoder.layers.0.self_attn.q_proj.weight', 'language_model.model.decoder.layers.0.self_attn.v_proj.bias', 'language_model.model.decoder.layers.0.self_attn.v_proj.weight', 'language_model.model.decoder.layers.0.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.0.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.1.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.1.fc1.bias', 'language_model.model.decoder.layers.1.fc1.weight', 'language_model.model.decoder.layers.1.fc2.bias', 'language_model.model.decoder.layers.1.fc2.weight', 'language_model.model.decoder.layers.1.final_layer_norm.bias', 'language_model.model.decoder.layers.1.final_layer_norm.weight', 'language_model.model.decoder.layers.1.self_attn.k_proj.bias', 'language_model.model.decoder.layers.1.self_attn.k_proj.weight', 'language_model.model.decoder.layers.1.self_attn.out_proj.bias', 'language_model.model.decoder.layers.1.self_attn.out_proj.weight', 'language_model.model.decoder.layers.1.self_attn.q_proj.bias', 'language_model.model.decoder.layers.1.self_attn.q_proj.weight', 'language_model.model.decoder.layers.1.self_attn.v_proj.bias', 'language_model.model.decoder.layers.1.self_attn.v_proj.weight', 'language_model.model.decoder.layers.1.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.1.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.2.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.2.fc1.bias', 'language_model.model.decoder.layers.2.fc1.weight', 'language_model.model.decoder.layers.2.fc2.bias', 'language_model.model.decoder.layers.2.fc2.weight', 'language_model.model.decoder.layers.2.final_layer_norm.bias', 'language_model.model.decoder.layers.2.final_layer_norm.weight', 'language_model.model.decoder.layers.2.self_attn.k_proj.bias', 'language_model.model.decoder.layers.2.self_attn.k_proj.weight', 'language_model.model.decoder.layers.2.self_attn.out_proj.bias', 'language_model.model.decoder.layers.2.self_attn.out_proj.weight', 'language_model.model.decoder.layers.2.self_attn.q_proj.bias', 'language_model.model.decoder.layers.2.self_attn.q_proj.weight', 'language_model.model.decoder.layers.2.self_attn.v_proj.bias', 'language_model.model.decoder.layers.2.self_attn.v_proj.weight', 'language_model.model.decoder.layers.2.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.2.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.3.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.3.fc1.bias', 'language_model.model.decoder.layers.3.fc1.weight', 'language_model.model.decoder.layers.3.fc2.bias', 'language_model.model.decoder.layers.3.fc2.weight', 'language_model.model.decoder.layers.3.final_layer_norm.bias', 'language_model.model.decoder.layers.3.final_layer_norm.weight', 'language_model.model.decoder.layers.3.self_attn.k_proj.bias', 'language_model.model.decoder.layers.3.self_attn.k_proj.weight', 'language_model.model.decoder.layers.3.self_attn.out_proj.bias', 'language_model.model.decoder.layers.3.self_attn.out_proj.weight', 'language_model.model.decoder.layers.3.self_attn.q_proj.bias', 'language_model.model.decoder.layers.3.self_attn.q_proj.weight', 'language_model.model.decoder.layers.3.self_attn.v_proj.bias', 'language_model.model.decoder.layers.3.self_attn.v_proj.weight', 'language_model.model.decoder.layers.3.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.3.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.4.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.4.fc1.bias', 'language_model.model.decoder.layers.4.fc1.weight', 'language_model.model.decoder.layers.4.fc2.bias', 'language_model.model.decoder.layers.4.fc2.weight', 'language_model.model.decoder.layers.4.final_layer_norm.bias', 'language_model.model.decoder.layers.4.final_layer_norm.weight', 'language_model.model.decoder.layers.4.self_attn.k_proj.bias', 'language_model.model.decoder.layers.4.self_attn.k_proj.weight', 'language_model.model.decoder.layers.4.self_attn.out_proj.bias', 'language_model.model.decoder.layers.4.self_attn.out_proj.weight', 'language_model.model.decoder.layers.4.self_attn.q_proj.bias', 'language_model.model.decoder.layers.4.self_attn.q_proj.weight', 'language_model.model.decoder.layers.4.self_attn.v_proj.bias', 'language_model.model.decoder.layers.4.self_attn.v_proj.weight', 'language_model.model.decoder.layers.4.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.4.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.fc1.bias', 'language_model.model.decoder.layers.5.fc1.weight', 'language_model.model.decoder.layers.5.fc2.bias', 'language_model.model.decoder.layers.5.fc2.weight', 'language_model.model.decoder.layers.5.final_layer_norm.bias', 'language_model.model.decoder.layers.5.final_layer_norm.weight', 'language_model.model.decoder.layers.5.self_attn.k_proj.bias', 'language_model.model.decoder.layers.5.self_attn.k_proj.weight', 'language_model.model.decoder.layers.5.self_attn.out_proj.bias', 'language_model.model.decoder.layers.5.self_attn.out_proj.weight', 'language_model.model.decoder.layers.5.self_attn.q_proj.bias', 'language_model.model.decoder.layers.5.self_attn.q_proj.weight', 'language_model.model.decoder.layers.5.self_attn.v_proj.bias', 'language_model.model.decoder.layers.5.self_attn.v_proj.weight', 'language_model.model.decoder.layers.5.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.5.self_attn_layer_norm.weight', 'language_model.model.encoder.embed_positions.weight', 'language_model.model.encoder.layernorm_embedding.bias', 'language_model.model.encoder.layernorm_embedding.weight', 'language_model.model.encoder.layers.0.fc1.bias', 'language_model.model.encoder.layers.0.fc1.weight', 'language_model.model.encoder.layers.0.fc2.bias', 'language_model.model.encoder.layers.0.fc2.weight', 'language_model.model.encoder.layers.0.final_layer_norm.bias', 'language_model.model.encoder.layers.0.final_layer_norm.weight', 'language_model.model.encoder.layers.0.self_attn.k_proj.bias', 'language_model.model.encoder.layers.0.self_attn.k_proj.weight', 'language_model.model.encoder.layers.0.self_attn.out_proj.bias', 'language_model.model.encoder.layers.0.self_attn.out_proj.weight', 'language_model.model.encoder.layers.0.self_attn.q_proj.bias', 'language_model.model.encoder.layers.0.self_attn.q_proj.weight', 'language_model.model.encoder.layers.0.self_attn.v_proj.bias', 'language_model.model.encoder.layers.0.self_attn.v_proj.weight', 'language_model.model.encoder.layers.0.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.0.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.1.fc1.bias', 'language_model.model.encoder.layers.1.fc1.weight', 'language_model.model.encoder.layers.1.fc2.bias', 'language_model.model.encoder.layers.1.fc2.weight', 'language_model.model.encoder.layers.1.final_layer_norm.bias', 'language_model.model.encoder.layers.1.final_layer_norm.weight', 'language_model.model.encoder.layers.1.self_attn.k_proj.bias', 'language_model.model.encoder.layers.1.self_attn.k_proj.weight', 'language_model.model.encoder.layers.1.self_attn.out_proj.bias', 'language_model.model.encoder.layers.1.self_attn.out_proj.weight', 'language_model.model.encoder.layers.1.self_attn.q_proj.bias', 'language_model.model.encoder.layers.1.self_attn.q_proj.weight', 'language_model.model.encoder.layers.1.self_attn.v_proj.bias', 'language_model.model.encoder.layers.1.self_attn.v_proj.weight', 'language_model.model.encoder.layers.1.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.1.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.2.fc1.bias', 'language_model.model.encoder.layers.2.fc1.weight', 'language_model.model.encoder.layers.2.fc2.bias', 'language_model.model.encoder.layers.2.fc2.weight', 'language_model.model.encoder.layers.2.final_layer_norm.bias', 'language_model.model.encoder.layers.2.final_layer_norm.weight', 'language_model.model.encoder.layers.2.self_attn.k_proj.bias', 'language_model.model.encoder.layers.2.self_attn.k_proj.weight', 'language_model.model.encoder.layers.2.self_attn.out_proj.bias', 'language_model.model.encoder.layers.2.self_attn.out_proj.weight', 'language_model.model.encoder.layers.2.self_attn.q_proj.bias', 'language_model.model.encoder.layers.2.self_attn.q_proj.weight', 'language_model.model.encoder.layers.2.self_attn.v_proj.bias', 'language_model.model.encoder.layers.2.self_attn.v_proj.weight', 'language_model.model.encoder.layers.2.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.2.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.3.fc1.bias', 'language_model.model.encoder.layers.3.fc1.weight', 'language_model.model.encoder.layers.3.fc2.bias', 'language_model.model.encoder.layers.3.fc2.weight', 'language_model.model.encoder.layers.3.final_layer_norm.bias', 'language_model.model.encoder.layers.3.final_layer_norm.weight', 'language_model.model.encoder.layers.3.self_attn.k_proj.bias', 'language_model.model.encoder.layers.3.self_attn.k_proj.weight', 'language_model.model.encoder.layers.3.self_attn.out_proj.bias', 'language_model.model.encoder.layers.3.self_attn.out_proj.weight', 'language_model.model.encoder.layers.3.self_attn.q_proj.bias', 'language_model.model.encoder.layers.3.self_attn.q_proj.weight', 'language_model.model.encoder.layers.3.self_attn.v_proj.bias', 'language_model.model.encoder.layers.3.self_attn.v_proj.weight', 'language_model.model.encoder.layers.3.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.3.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.4.fc1.bias', 'language_model.model.encoder.layers.4.fc1.weight', 'language_model.model.encoder.layers.4.fc2.bias', 'language_model.model.encoder.layers.4.fc2.weight', 'language_model.model.encoder.layers.4.final_layer_norm.bias', 'language_model.model.encoder.layers.4.final_layer_norm.weight', 'language_model.model.encoder.layers.4.self_attn.k_proj.bias', 'language_model.model.encoder.layers.4.self_attn.k_proj.weight', 'language_model.model.encoder.layers.4.self_attn.out_proj.bias', 'language_model.model.encoder.layers.4.self_attn.out_proj.weight', 'language_model.model.encoder.layers.4.self_attn.q_proj.bias', 'language_model.model.encoder.layers.4.self_attn.q_proj.weight', 'language_model.model.encoder.layers.4.self_attn.v_proj.bias', 'language_model.model.encoder.layers.4.self_attn.v_proj.weight', 'language_model.model.encoder.layers.4.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.4.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.5.fc1.bias', 'language_model.model.encoder.layers.5.fc1.weight', 'language_model.model.encoder.layers.5.fc2.bias', 'language_model.model.encoder.layers.5.fc2.weight', 'language_model.model.encoder.layers.5.final_layer_norm.bias', 'language_model.model.encoder.layers.5.final_layer_norm.weight', 'language_model.model.encoder.layers.5.self_attn.k_proj.bias', 'language_model.model.encoder.layers.5.self_attn.k_proj.weight', 'language_model.model.encoder.layers.5.self_attn.out_proj.bias', 'language_model.model.encoder.layers.5.self_attn.out_proj.weight', 'language_model.model.encoder.layers.5.self_attn.q_proj.bias', 'language_model.model.encoder.layers.5.self_attn.q_proj.weight', 'language_model.model.encoder.layers.5.self_attn.v_proj.bias', 'language_model.model.encoder.layers.5.self_attn.v_proj.weight', 'language_model.model.encoder.layers.5.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.5.self_attn_layer_norm.weight', 'language_model.model.shared.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.0.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.0.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.0.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.0.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.0.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.0.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.1.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.1.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.1.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.1.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.1.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.1.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.1.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.1.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.1.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.1.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.1.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.1.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.2.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.2.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.2.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.2.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.2.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.2.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.3.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.3.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.3.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.3.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.3.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.3.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.4.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.4.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.4.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.4.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.4.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.4.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.5.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.5.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.5.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.5.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.5.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.5.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.6.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.6.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.6.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.6.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.6.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.6.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.7.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.7.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.7.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.7.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.7.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.7.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.8.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.8.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.8.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.8.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.8.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.8.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.3.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.3.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.3.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.3.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.3.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.3.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.norm.weight', 'vision_tower.convs.0.norm.bias', 'vision_tower.convs.0.norm.weight', 'vision_tower.convs.0.proj.bias', 'vision_tower.convs.0.proj.weight', 'vision_tower.convs.1.norm.bias', 'vision_tower.convs.1.norm.weight', 'vision_tower.convs.1.proj.bias', 'vision_tower.convs.1.proj.weight', 'vision_tower.convs.2.norm.bias', 'vision_tower.convs.2.norm.weight', 'vision_tower.convs.2.proj.bias', 'vision_tower.convs.2.proj.weight', 'vision_tower.convs.3.norm.bias', 'vision_tower.convs.3.norm.weight', 'vision_tower.convs.3.proj.bias', 'vision_tower.convs.3.proj.weight', 'visual_temporal_embed.pos_idx_to_embed']\n",
            "- This IS expected if you are initializing Florence2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Florence2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Florence2ForConditionalGeneration were not initialized from the model checkpoint at microsoft/Florence-2-base and are newly initialized: ['lm_head.weight', 'model.language_model.decoder.embed_positions.weight', 'model.language_model.decoder.embed_tokens.weight', 'model.language_model.decoder.layernorm_embedding.bias', 'model.language_model.decoder.layernorm_embedding.weight', 'model.language_model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.0.fc1.bias', 'model.language_model.decoder.layers.0.fc1.weight', 'model.language_model.decoder.layers.0.fc2.bias', 'model.language_model.decoder.layers.0.fc2.weight', 'model.language_model.decoder.layers.0.final_layer_norm.bias', 'model.language_model.decoder.layers.0.final_layer_norm.weight', 'model.language_model.decoder.layers.0.self_attn.k_proj.bias', 'model.language_model.decoder.layers.0.self_attn.k_proj.weight', 'model.language_model.decoder.layers.0.self_attn.out_proj.bias', 'model.language_model.decoder.layers.0.self_attn.out_proj.weight', 'model.language_model.decoder.layers.0.self_attn.q_proj.bias', 'model.language_model.decoder.layers.0.self_attn.q_proj.weight', 'model.language_model.decoder.layers.0.self_attn.v_proj.bias', 'model.language_model.decoder.layers.0.self_attn.v_proj.weight', 'model.language_model.decoder.layers.0.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.0.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.1.fc1.bias', 'model.language_model.decoder.layers.1.fc1.weight', 'model.language_model.decoder.layers.1.fc2.bias', 'model.language_model.decoder.layers.1.fc2.weight', 'model.language_model.decoder.layers.1.final_layer_norm.bias', 'model.language_model.decoder.layers.1.final_layer_norm.weight', 'model.language_model.decoder.layers.1.self_attn.k_proj.bias', 'model.language_model.decoder.layers.1.self_attn.k_proj.weight', 'model.language_model.decoder.layers.1.self_attn.out_proj.bias', 'model.language_model.decoder.layers.1.self_attn.out_proj.weight', 'model.language_model.decoder.layers.1.self_attn.q_proj.bias', 'model.language_model.decoder.layers.1.self_attn.q_proj.weight', 'model.language_model.decoder.layers.1.self_attn.v_proj.bias', 'model.language_model.decoder.layers.1.self_attn.v_proj.weight', 'model.language_model.decoder.layers.1.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.1.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.2.fc1.bias', 'model.language_model.decoder.layers.2.fc1.weight', 'model.language_model.decoder.layers.2.fc2.bias', 'model.language_model.decoder.layers.2.fc2.weight', 'model.language_model.decoder.layers.2.final_layer_norm.bias', 'model.language_model.decoder.layers.2.final_layer_norm.weight', 'model.language_model.decoder.layers.2.self_attn.k_proj.bias', 'model.language_model.decoder.layers.2.self_attn.k_proj.weight', 'model.language_model.decoder.layers.2.self_attn.out_proj.bias', 'model.language_model.decoder.layers.2.self_attn.out_proj.weight', 'model.language_model.decoder.layers.2.self_attn.q_proj.bias', 'model.language_model.decoder.layers.2.self_attn.q_proj.weight', 'model.language_model.decoder.layers.2.self_attn.v_proj.bias', 'model.language_model.decoder.layers.2.self_attn.v_proj.weight', 'model.language_model.decoder.layers.2.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.2.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.3.fc1.bias', 'model.language_model.decoder.layers.3.fc1.weight', 'model.language_model.decoder.layers.3.fc2.bias', 'model.language_model.decoder.layers.3.fc2.weight', 'model.language_model.decoder.layers.3.final_layer_norm.bias', 'model.language_model.decoder.layers.3.final_layer_norm.weight', 'model.language_model.decoder.layers.3.self_attn.k_proj.bias', 'model.language_model.decoder.layers.3.self_attn.k_proj.weight', 'model.language_model.decoder.layers.3.self_attn.out_proj.bias', 'model.language_model.decoder.layers.3.self_attn.out_proj.weight', 'model.language_model.decoder.layers.3.self_attn.q_proj.bias', 'model.language_model.decoder.layers.3.self_attn.q_proj.weight', 'model.language_model.decoder.layers.3.self_attn.v_proj.bias', 'model.language_model.decoder.layers.3.self_attn.v_proj.weight', 'model.language_model.decoder.layers.3.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.3.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.4.fc1.bias', 'model.language_model.decoder.layers.4.fc1.weight', 'model.language_model.decoder.layers.4.fc2.bias', 'model.language_model.decoder.layers.4.fc2.weight', 'model.language_model.decoder.layers.4.final_layer_norm.bias', 'model.language_model.decoder.layers.4.final_layer_norm.weight', 'model.language_model.decoder.layers.4.self_attn.k_proj.bias', 'model.language_model.decoder.layers.4.self_attn.k_proj.weight', 'model.language_model.decoder.layers.4.self_attn.out_proj.bias', 'model.language_model.decoder.layers.4.self_attn.out_proj.weight', 'model.language_model.decoder.layers.4.self_attn.q_proj.bias', 'model.language_model.decoder.layers.4.self_attn.q_proj.weight', 'model.language_model.decoder.layers.4.self_attn.v_proj.bias', 'model.language_model.decoder.layers.4.self_attn.v_proj.weight', 'model.language_model.decoder.layers.4.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.4.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.5.fc1.bias', 'model.language_model.decoder.layers.5.fc1.weight', 'model.language_model.decoder.layers.5.fc2.bias', 'model.language_model.decoder.layers.5.fc2.weight', 'model.language_model.decoder.layers.5.final_layer_norm.bias', 'model.language_model.decoder.layers.5.final_layer_norm.weight', 'model.language_model.decoder.layers.5.self_attn.k_proj.bias', 'model.language_model.decoder.layers.5.self_attn.k_proj.weight', 'model.language_model.decoder.layers.5.self_attn.out_proj.bias', 'model.language_model.decoder.layers.5.self_attn.out_proj.weight', 'model.language_model.decoder.layers.5.self_attn.q_proj.bias', 'model.language_model.decoder.layers.5.self_attn.q_proj.weight', 'model.language_model.decoder.layers.5.self_attn.v_proj.bias', 'model.language_model.decoder.layers.5.self_attn.v_proj.weight', 'model.language_model.decoder.layers.5.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.5.self_attn_layer_norm.weight', 'model.language_model.encoder.embed_positions.weight', 'model.language_model.encoder.embed_tokens.weight', 'model.language_model.encoder.layernorm_embedding.bias', 'model.language_model.encoder.layernorm_embedding.weight', 'model.language_model.encoder.layers.0.fc1.bias', 'model.language_model.encoder.layers.0.fc1.weight', 'model.language_model.encoder.layers.0.fc2.bias', 'model.language_model.encoder.layers.0.fc2.weight', 'model.language_model.encoder.layers.0.final_layer_norm.bias', 'model.language_model.encoder.layers.0.final_layer_norm.weight', 'model.language_model.encoder.layers.0.self_attn.k_proj.bias', 'model.language_model.encoder.layers.0.self_attn.k_proj.weight', 'model.language_model.encoder.layers.0.self_attn.out_proj.bias', 'model.language_model.encoder.layers.0.self_attn.out_proj.weight', 'model.language_model.encoder.layers.0.self_attn.q_proj.bias', 'model.language_model.encoder.layers.0.self_attn.q_proj.weight', 'model.language_model.encoder.layers.0.self_attn.v_proj.bias', 'model.language_model.encoder.layers.0.self_attn.v_proj.weight', 'model.language_model.encoder.layers.0.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.0.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.1.fc1.bias', 'model.language_model.encoder.layers.1.fc1.weight', 'model.language_model.encoder.layers.1.fc2.bias', 'model.language_model.encoder.layers.1.fc2.weight', 'model.language_model.encoder.layers.1.final_layer_norm.bias', 'model.language_model.encoder.layers.1.final_layer_norm.weight', 'model.language_model.encoder.layers.1.self_attn.k_proj.bias', 'model.language_model.encoder.layers.1.self_attn.k_proj.weight', 'model.language_model.encoder.layers.1.self_attn.out_proj.bias', 'model.language_model.encoder.layers.1.self_attn.out_proj.weight', 'model.language_model.encoder.layers.1.self_attn.q_proj.bias', 'model.language_model.encoder.layers.1.self_attn.q_proj.weight', 'model.language_model.encoder.layers.1.self_attn.v_proj.bias', 'model.language_model.encoder.layers.1.self_attn.v_proj.weight', 'model.language_model.encoder.layers.1.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.1.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.2.fc1.bias', 'model.language_model.encoder.layers.2.fc1.weight', 'model.language_model.encoder.layers.2.fc2.bias', 'model.language_model.encoder.layers.2.fc2.weight', 'model.language_model.encoder.layers.2.final_layer_norm.bias', 'model.language_model.encoder.layers.2.final_layer_norm.weight', 'model.language_model.encoder.layers.2.self_attn.k_proj.bias', 'model.language_model.encoder.layers.2.self_attn.k_proj.weight', 'model.language_model.encoder.layers.2.self_attn.out_proj.bias', 'model.language_model.encoder.layers.2.self_attn.out_proj.weight', 'model.language_model.encoder.layers.2.self_attn.q_proj.bias', 'model.language_model.encoder.layers.2.self_attn.q_proj.weight', 'model.language_model.encoder.layers.2.self_attn.v_proj.bias', 'model.language_model.encoder.layers.2.self_attn.v_proj.weight', 'model.language_model.encoder.layers.2.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.2.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.3.fc1.bias', 'model.language_model.encoder.layers.3.fc1.weight', 'model.language_model.encoder.layers.3.fc2.bias', 'model.language_model.encoder.layers.3.fc2.weight', 'model.language_model.encoder.layers.3.final_layer_norm.bias', 'model.language_model.encoder.layers.3.final_layer_norm.weight', 'model.language_model.encoder.layers.3.self_attn.k_proj.bias', 'model.language_model.encoder.layers.3.self_attn.k_proj.weight', 'model.language_model.encoder.layers.3.self_attn.out_proj.bias', 'model.language_model.encoder.layers.3.self_attn.out_proj.weight', 'model.language_model.encoder.layers.3.self_attn.q_proj.bias', 'model.language_model.encoder.layers.3.self_attn.q_proj.weight', 'model.language_model.encoder.layers.3.self_attn.v_proj.bias', 'model.language_model.encoder.layers.3.self_attn.v_proj.weight', 'model.language_model.encoder.layers.3.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.3.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.4.fc1.bias', 'model.language_model.encoder.layers.4.fc1.weight', 'model.language_model.encoder.layers.4.fc2.bias', 'model.language_model.encoder.layers.4.fc2.weight', 'model.language_model.encoder.layers.4.final_layer_norm.bias', 'model.language_model.encoder.layers.4.final_layer_norm.weight', 'model.language_model.encoder.layers.4.self_attn.k_proj.bias', 'model.language_model.encoder.layers.4.self_attn.k_proj.weight', 'model.language_model.encoder.layers.4.self_attn.out_proj.bias', 'model.language_model.encoder.layers.4.self_attn.out_proj.weight', 'model.language_model.encoder.layers.4.self_attn.q_proj.bias', 'model.language_model.encoder.layers.4.self_attn.q_proj.weight', 'model.language_model.encoder.layers.4.self_attn.v_proj.bias', 'model.language_model.encoder.layers.4.self_attn.v_proj.weight', 'model.language_model.encoder.layers.4.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.4.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.5.fc1.bias', 'model.language_model.encoder.layers.5.fc1.weight', 'model.language_model.encoder.layers.5.fc2.bias', 'model.language_model.encoder.layers.5.fc2.weight', 'model.language_model.encoder.layers.5.final_layer_norm.bias', 'model.language_model.encoder.layers.5.final_layer_norm.weight', 'model.language_model.encoder.layers.5.self_attn.k_proj.bias', 'model.language_model.encoder.layers.5.self_attn.k_proj.weight', 'model.language_model.encoder.layers.5.self_attn.out_proj.bias', 'model.language_model.encoder.layers.5.self_attn.out_proj.weight', 'model.language_model.encoder.layers.5.self_attn.q_proj.bias', 'model.language_model.encoder.layers.5.self_attn.q_proj.weight', 'model.language_model.encoder.layers.5.self_attn.v_proj.bias', 'model.language_model.encoder.layers.5.self_attn.v_proj.weight', 'model.language_model.encoder.layers.5.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.5.self_attn_layer_norm.weight', 'model.language_model.shared.weight', 'model.multi_modal_projector.image_position_embed.column_embeddings.weight', 'model.multi_modal_projector.image_position_embed.row_embeddings.weight', 'model.multi_modal_projector.image_proj_norm.bias', 'model.multi_modal_projector.image_proj_norm.weight', 'model.multi_modal_projector.image_projection.weight', 'model.multi_modal_projector.visual_temporal_embed.pos_idx_to_embed', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.0.0.channel_block.conv1.bias', 'model.vision_tower.blocks.0.0.channel_block.conv1.weight', 'model.vision_tower.blocks.0.0.channel_block.conv2.bias', 'model.vision_tower.blocks.0.0.channel_block.conv2.weight', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.0.0.channel_block.norm1.bias', 'model.vision_tower.blocks.0.0.channel_block.norm1.weight', 'model.vision_tower.blocks.0.0.channel_block.norm2.bias', 'model.vision_tower.blocks.0.0.channel_block.norm2.weight', 'model.vision_tower.blocks.0.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.0.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.0.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.0.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.0.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.0.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.0.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.0.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.1.0.channel_block.conv1.bias', 'model.vision_tower.blocks.1.0.channel_block.conv1.weight', 'model.vision_tower.blocks.1.0.channel_block.conv2.bias', 'model.vision_tower.blocks.1.0.channel_block.conv2.weight', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.1.0.channel_block.norm1.bias', 'model.vision_tower.blocks.1.0.channel_block.norm1.weight', 'model.vision_tower.blocks.1.0.channel_block.norm2.bias', 'model.vision_tower.blocks.1.0.channel_block.norm2.weight', 'model.vision_tower.blocks.1.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.1.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.1.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.1.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.1.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.1.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.1.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.1.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.0.channel_block.conv1.bias', 'model.vision_tower.blocks.2.0.channel_block.conv1.weight', 'model.vision_tower.blocks.2.0.channel_block.conv2.bias', 'model.vision_tower.blocks.2.0.channel_block.conv2.weight', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.0.channel_block.norm1.bias', 'model.vision_tower.blocks.2.0.channel_block.norm1.weight', 'model.vision_tower.blocks.2.0.channel_block.norm2.bias', 'model.vision_tower.blocks.2.0.channel_block.norm2.weight', 'model.vision_tower.blocks.2.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.1.channel_block.conv1.bias', 'model.vision_tower.blocks.2.1.channel_block.conv1.weight', 'model.vision_tower.blocks.2.1.channel_block.conv2.bias', 'model.vision_tower.blocks.2.1.channel_block.conv2.weight', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.1.channel_block.norm1.bias', 'model.vision_tower.blocks.2.1.channel_block.norm1.weight', 'model.vision_tower.blocks.2.1.channel_block.norm2.bias', 'model.vision_tower.blocks.2.1.channel_block.norm2.weight', 'model.vision_tower.blocks.2.1.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.1.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.1.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.1.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.1.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.1.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.1.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.1.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.2.channel_block.conv1.bias', 'model.vision_tower.blocks.2.2.channel_block.conv1.weight', 'model.vision_tower.blocks.2.2.channel_block.conv2.bias', 'model.vision_tower.blocks.2.2.channel_block.conv2.weight', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.2.channel_block.norm1.bias', 'model.vision_tower.blocks.2.2.channel_block.norm1.weight', 'model.vision_tower.blocks.2.2.channel_block.norm2.bias', 'model.vision_tower.blocks.2.2.channel_block.norm2.weight', 'model.vision_tower.blocks.2.2.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.2.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.2.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.2.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.2.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.2.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.2.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.2.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.3.channel_block.conv1.bias', 'model.vision_tower.blocks.2.3.channel_block.conv1.weight', 'model.vision_tower.blocks.2.3.channel_block.conv2.bias', 'model.vision_tower.blocks.2.3.channel_block.conv2.weight', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.3.channel_block.norm1.bias', 'model.vision_tower.blocks.2.3.channel_block.norm1.weight', 'model.vision_tower.blocks.2.3.channel_block.norm2.bias', 'model.vision_tower.blocks.2.3.channel_block.norm2.weight', 'model.vision_tower.blocks.2.3.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.3.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.3.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.3.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.3.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.3.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.3.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.3.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.4.channel_block.conv1.bias', 'model.vision_tower.blocks.2.4.channel_block.conv1.weight', 'model.vision_tower.blocks.2.4.channel_block.conv2.bias', 'model.vision_tower.blocks.2.4.channel_block.conv2.weight', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.4.channel_block.norm1.bias', 'model.vision_tower.blocks.2.4.channel_block.norm1.weight', 'model.vision_tower.blocks.2.4.channel_block.norm2.bias', 'model.vision_tower.blocks.2.4.channel_block.norm2.weight', 'model.vision_tower.blocks.2.4.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.4.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.4.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.4.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.4.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.4.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.4.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.4.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.5.channel_block.conv1.bias', 'model.vision_tower.blocks.2.5.channel_block.conv1.weight', 'model.vision_tower.blocks.2.5.channel_block.conv2.bias', 'model.vision_tower.blocks.2.5.channel_block.conv2.weight', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.5.channel_block.norm1.bias', 'model.vision_tower.blocks.2.5.channel_block.norm1.weight', 'model.vision_tower.blocks.2.5.channel_block.norm2.bias', 'model.vision_tower.blocks.2.5.channel_block.norm2.weight', 'model.vision_tower.blocks.2.5.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.5.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.5.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.5.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.5.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.5.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.5.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.5.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.6.channel_block.conv1.bias', 'model.vision_tower.blocks.2.6.channel_block.conv1.weight', 'model.vision_tower.blocks.2.6.channel_block.conv2.bias', 'model.vision_tower.blocks.2.6.channel_block.conv2.weight', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.6.channel_block.norm1.bias', 'model.vision_tower.blocks.2.6.channel_block.norm1.weight', 'model.vision_tower.blocks.2.6.channel_block.norm2.bias', 'model.vision_tower.blocks.2.6.channel_block.norm2.weight', 'model.vision_tower.blocks.2.6.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.6.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.6.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.6.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.6.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.6.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.6.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.6.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.7.channel_block.conv1.bias', 'model.vision_tower.blocks.2.7.channel_block.conv1.weight', 'model.vision_tower.blocks.2.7.channel_block.conv2.bias', 'model.vision_tower.blocks.2.7.channel_block.conv2.weight', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.7.channel_block.norm1.bias', 'model.vision_tower.blocks.2.7.channel_block.norm1.weight', 'model.vision_tower.blocks.2.7.channel_block.norm2.bias', 'model.vision_tower.blocks.2.7.channel_block.norm2.weight', 'model.vision_tower.blocks.2.7.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.7.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.7.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.7.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.7.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.7.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.7.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.7.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.8.channel_block.conv1.bias', 'model.vision_tower.blocks.2.8.channel_block.conv1.weight', 'model.vision_tower.blocks.2.8.channel_block.conv2.bias', 'model.vision_tower.blocks.2.8.channel_block.conv2.weight', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.8.channel_block.norm1.bias', 'model.vision_tower.blocks.2.8.channel_block.norm1.weight', 'model.vision_tower.blocks.2.8.channel_block.norm2.bias', 'model.vision_tower.blocks.2.8.channel_block.norm2.weight', 'model.vision_tower.blocks.2.8.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.8.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.8.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.8.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.8.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.8.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.8.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.8.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.3.0.channel_block.conv1.bias', 'model.vision_tower.blocks.3.0.channel_block.conv1.weight', 'model.vision_tower.blocks.3.0.channel_block.conv2.bias', 'model.vision_tower.blocks.3.0.channel_block.conv2.weight', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.3.0.channel_block.norm1.bias', 'model.vision_tower.blocks.3.0.channel_block.norm1.weight', 'model.vision_tower.blocks.3.0.channel_block.norm2.bias', 'model.vision_tower.blocks.3.0.channel_block.norm2.weight', 'model.vision_tower.blocks.3.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.3.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.3.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.3.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.3.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.3.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.3.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.3.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.convs.0.conv.bias', 'model.vision_tower.convs.0.conv.weight', 'model.vision_tower.convs.0.norm.bias', 'model.vision_tower.convs.0.norm.weight', 'model.vision_tower.convs.1.conv.bias', 'model.vision_tower.convs.1.conv.weight', 'model.vision_tower.convs.1.norm.bias', 'model.vision_tower.convs.1.norm.weight', 'model.vision_tower.convs.2.conv.bias', 'model.vision_tower.convs.2.conv.weight', 'model.vision_tower.convs.2.norm.bias', 'model.vision_tower.convs.2.norm.weight', 'model.vision_tower.convs.3.conv.bias', 'model.vision_tower.convs.3.conv.weight', 'model.vision_tower.convs.3.norm.bias', 'model.vision_tower.convs.3.norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, Florence2ForConditionalGeneration\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n",
        "model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pknaF9kZmZSb",
        "outputId": "32689612-f887-4849-9db6-83a6fbba397f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.57.1\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.40 #florence2 not supported for transformers >= 4.49. Requires 4.40 at least to run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AyKgGHfvmmut",
        "outputId": "61e880a9-07f4-46bd-944a-3e53bc2412e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/137.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (2.32.4)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40) (2025.11.12)\n",
            "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example code -- pipeline\n",
        "# Use a pipeline as a high-level helper\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"microsoft/Florence-2-base\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kXobvAjvnLAf",
        "outputId": "f3b3ac42-a6c2-4e6d-846a-e87e2259f143"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch' has no attribute 'distributed' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-945325303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#example code -- pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use a pipeline as a high-level helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from torch.nn import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m from .activation import (\n\u001b[1;32m      4\u001b[0m     \u001b[0mCELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_int\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sym_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._jit_internal import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_overload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mboolean_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_rpc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyRRef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRRef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'distributed' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoProcessor, Florence2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "transformers.logging.set_verbosity_error() #gets rid of error messages\n",
        "# Load model directly\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n",
        "model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n",
        "\n",
        "image = Image.open(\"R2D2_1.jpg\")\n",
        "\n",
        "inputs = processor(text=\"<ITS><image>Describe the image\", images=image, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(processor.decode(outputs[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Njr4e0nwty8A",
        "outputId": "3e1b6931-96c7-484f-f73d-672677f17fe6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Image features and image tokens do not match: tokens: 0, features 577",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2607727370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<ITS><image>Describe the image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m             special_image_mask = self.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1037\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     ):\n\u001b[0;32m-> 1015\u001b[0;31m         return self.model.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mn_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecial_image_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    708\u001b[0m                 \u001b[0;34mf\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Image features and image tokens do not match: tokens: 0, features 577"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoProcessor, Florence2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "# Convert image to RGB and print mode/size\n",
        "image = Image.open(\"R2D2_1.jpg\")\n",
        "print(\"Image mode / size:\", image.mode, image.size)\n",
        "if image.mode != \"RGB\":\n",
        "    image = image.convert(\"RGB\")\n",
        "    print(\"Converted to RGB\")\n",
        "\n",
        "# load with trust_remote_code\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n",
        "model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n",
        "\n",
        "# Use batch form (lists)  this often avoids token/feature mismatch\n",
        "prompt = \"<image><ITS>Describe the image.\"\n",
        "texts = [prompt]\n",
        "images = [image]\n",
        "\n",
        "inputs = processor(text=texts, images=images, return_tensors=\"pt\")\n",
        "\n",
        "# Print returned keys and shapes / lengths\n",
        "print(\"\\nPROCESSOR OUTPUT KEYS:\")\n",
        "for k, v in inputs.items():\n",
        "    try:\n",
        "        print(f\"  {k}: type={type(v)}, shape={getattr(v, 'shape', 'n/a')}, len(if tensor)={(v.shape if hasattr(v,'shape') else 'n/a')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {k}: (error printing) {e}\")\n",
        "\n",
        "# If there is an 'input_ids' inspect tokens\n",
        "if \"input_ids\" in inputs:\n",
        "    print(\"\\ninput_ids shape:\", inputs[\"input_ids\"].shape)\n",
        "    # Show first 60 token ids and the decoded text (short)\n",
        "    print(\"input_ids[0][:60]:\", inputs[\"input_ids\"][0][:60].tolist())\n",
        "    try:\n",
        "        print(\"decoded prompt:\", processor.tokenizer.decode(inputs[\"input_ids\"][0].tolist(), skip_special_tokens=False))\n",
        "    except Exception as e:\n",
        "        print(\"Could not decode input_ids:\", e)\n",
        "\n",
        "# If there is an 'image' or 'image_features' or 'pixel_values' key, show shape\n",
        "for k in (\"pixel_values\", \"image_features\", \"images\", \"image_embeds\"):\n",
        "    if k in inputs:\n",
        "        print(f\"{k} shape:\", getattr(inputs[k], \"shape\", \"n/a\"))\n",
        "\n",
        "# Try a quick forward pass to catch the same error and print stack\n",
        "try:\n",
        "    outputs = model.generate(**{k:v.to(model.device) if isinstance(v, torch.Tensor) else v for k,v in inputs.items()})\n",
        "    print(\"\\nGenerated (ids):\", outputs)\n",
        "    print(\"Decoded:\", processor.decode(outputs[0]))\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(\"\\n--- MODEL RUN ERROR ---\")\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baNIilaqu4YJ",
        "outputId": "0bd35abd-acbf-454a-f69d-bd2ceb0a211b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image mode / size: RGB (576, 484)\n",
            "\n",
            "PROCESSOR OUTPUT KEYS:\n",
            "  input_ids: type=<class 'torch.Tensor'>, shape=torch.Size([1, 12]), len(if tensor)=torch.Size([1, 12])\n",
            "  attention_mask: type=<class 'torch.Tensor'>, shape=torch.Size([1, 12]), len(if tensor)=torch.Size([1, 12])\n",
            "  pixel_values: type=<class 'torch.Tensor'>, shape=torch.Size([1, 3, 768, 768]), len(if tensor)=torch.Size([1, 3, 768, 768])\n",
            "\n",
            "input_ids shape: torch.Size([1, 12])\n",
            "input_ids[0][:60]: [0, 41552, 20094, 49007, 27510, 15698, 47066, 21700, 5, 2274, 4, 2]\n",
            "decoded prompt: <s><image><ITS>Describe the image.</s>\n",
            "pixel_values shape: torch.Size([1, 3, 768, 768])\n",
            "\n",
            "--- MODEL RUN ERROR ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1466727224.py\", line 51, in <cell line: 0>\n",
            "    outputs = model.generate(**{k:v.to(model.device) if isinstance(v, torch.Tensor) else v for k,v in inputs.items()})\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2465, in generate\n",
            "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\", line 1036, in _prepare_encoder_decoder_kwargs_for_generation\n",
            "    special_image_mask = self.get_placeholder_mask(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\", line 1015, in get_placeholder_mask\n",
            "    return self.model.get_placeholder_mask(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\", line 707, in get_placeholder_mask\n",
            "    raise ValueError(\n",
            "ValueError: Image features and image tokens do not match: tokens: 0, features 577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYBgVqH4vyQ6",
        "outputId": "7fdcf10f-7284-4a72-8ca4-c4842a74ddd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import scan_cache\n",
        "\n",
        "cache_info = scan_cache()\n",
        "print(cache_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "4ff5w9rnvtps",
        "outputId": "953f54b8-2d0c-4236-fa81-470920d09267"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'scan_cache' from 'huggingface_hub' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2506021138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcache_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'scan_cache' from 'huggingface_hub' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoProcessor, Florence2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"microsoft/Florence-2-base\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "processor.num_additional_image_tokens = 1\n",
        "\n",
        "model = Florence2ForConditionalGeneration.from_pretrained(\n",
        "    \"microsoft/Florence-2-base\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "image = Image.open(\"R2D2_1.jpg\")\n",
        "\n",
        "prompt = \"<ITS><image>Describe the image.\"\n",
        "\n",
        "inputs = processor(\n",
        "    text=prompt,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs)\n",
        "\n",
        "print(processor.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lEnwSaWnxLcf",
        "outputId": "3478fad7-11b0-4324-daf9-1cafd489efb5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Image features and image tokens do not match: tokens: 0, features 577",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-133671581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m ).to(model.device)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m             special_image_mask = self.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1037\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     ):\n\u001b[0;32m-> 1015\u001b[0;31m         return self.model.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mn_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecial_image_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    708\u001b[0m                 \u001b[0;34mf\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Image features and image tokens do not match: tokens: 0, features 577"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Florence2ForConditionalGeneration # Reverted to correct model class\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "model_id = \"microsoft/Florence-2-base\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Reverted to Florence2ForConditionalGeneration\n",
        "model = Florence2ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "image = Image.open(\"R2D2_1.jpg\")\n",
        "\n",
        "prompt = \"<CAPTION>\"\n",
        "\n",
        "# Removed the problematic image_token_id check, relying on processor to handle image tokens\n",
        "inputs = processor(\n",
        "    text=prompt,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=64\n",
        ")\n",
        "\n",
        "print(processor.decode(generated_ids[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "8fDpq3LszPTY",
        "outputId": "a9574a85-7cdc-4732-92fc-4b85b1bf90d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/Florence-2-base were not used when initializing Florence2ForConditionalGeneration: ['image_pos_embed.column_embeddings.weight', 'image_pos_embed.row_embeddings.weight', 'image_proj_norm.bias', 'image_proj_norm.weight', 'image_projection', 'language_model.final_logits_bias', 'language_model.model.decoder.embed_positions.weight', 'language_model.model.decoder.layernorm_embedding.bias', 'language_model.model.decoder.layernorm_embedding.weight', 'language_model.model.decoder.layers.0.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.0.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.0.fc1.bias', 'language_model.model.decoder.layers.0.fc1.weight', 'language_model.model.decoder.layers.0.fc2.bias', 'language_model.model.decoder.layers.0.fc2.weight', 'language_model.model.decoder.layers.0.final_layer_norm.bias', 'language_model.model.decoder.layers.0.final_layer_norm.weight', 'language_model.model.decoder.layers.0.self_attn.k_proj.bias', 'language_model.model.decoder.layers.0.self_attn.k_proj.weight', 'language_model.model.decoder.layers.0.self_attn.out_proj.bias', 'language_model.model.decoder.layers.0.self_attn.out_proj.weight', 'language_model.model.decoder.layers.0.self_attn.q_proj.bias', 'language_model.model.decoder.layers.0.self_attn.q_proj.weight', 'language_model.model.decoder.layers.0.self_attn.v_proj.bias', 'language_model.model.decoder.layers.0.self_attn.v_proj.weight', 'language_model.model.decoder.layers.0.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.0.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.1.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.1.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.1.fc1.bias', 'language_model.model.decoder.layers.1.fc1.weight', 'language_model.model.decoder.layers.1.fc2.bias', 'language_model.model.decoder.layers.1.fc2.weight', 'language_model.model.decoder.layers.1.final_layer_norm.bias', 'language_model.model.decoder.layers.1.final_layer_norm.weight', 'language_model.model.decoder.layers.1.self_attn.k_proj.bias', 'language_model.model.decoder.layers.1.self_attn.k_proj.weight', 'language_model.model.decoder.layers.1.self_attn.out_proj.bias', 'language_model.model.decoder.layers.1.self_attn.out_proj.weight', 'language_model.model.decoder.layers.1.self_attn.q_proj.bias', 'language_model.model.decoder.layers.1.self_attn.q_proj.weight', 'language_model.model.decoder.layers.1.self_attn.v_proj.bias', 'language_model.model.decoder.layers.1.self_attn.v_proj.weight', 'language_model.model.decoder.layers.1.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.1.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.2.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.2.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.2.fc1.bias', 'language_model.model.decoder.layers.2.fc1.weight', 'language_model.model.decoder.layers.2.fc2.bias', 'language_model.model.decoder.layers.2.fc2.weight', 'language_model.model.decoder.layers.2.final_layer_norm.bias', 'language_model.model.decoder.layers.2.final_layer_norm.weight', 'language_model.model.decoder.layers.2.self_attn.k_proj.bias', 'language_model.model.decoder.layers.2.self_attn.k_proj.weight', 'language_model.model.decoder.layers.2.self_attn.out_proj.bias', 'language_model.model.decoder.layers.2.self_attn.out_proj.weight', 'language_model.model.decoder.layers.2.self_attn.q_proj.bias', 'language_model.model.decoder.layers.2.self_attn.q_proj.weight', 'language_model.model.decoder.layers.2.self_attn.v_proj.bias', 'language_model.model.decoder.layers.2.self_attn.v_proj.weight', 'language_model.model.decoder.layers.2.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.2.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.3.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.3.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.3.fc1.bias', 'language_model.model.decoder.layers.3.fc1.weight', 'language_model.model.decoder.layers.3.fc2.bias', 'language_model.model.decoder.layers.3.fc2.weight', 'language_model.model.decoder.layers.3.final_layer_norm.bias', 'language_model.model.decoder.layers.3.final_layer_norm.weight', 'language_model.model.decoder.layers.3.self_attn.k_proj.bias', 'language_model.model.decoder.layers.3.self_attn.k_proj.weight', 'language_model.model.decoder.layers.3.self_attn.out_proj.bias', 'language_model.model.decoder.layers.3.self_attn.out_proj.weight', 'language_model.model.decoder.layers.3.self_attn.q_proj.bias', 'language_model.model.decoder.layers.3.self_attn.q_proj.weight', 'language_model.model.decoder.layers.3.self_attn.v_proj.bias', 'language_model.model.decoder.layers.3.self_attn.v_proj.weight', 'language_model.model.decoder.layers.3.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.3.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.4.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.4.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.4.fc1.bias', 'language_model.model.decoder.layers.4.fc1.weight', 'language_model.model.decoder.layers.4.fc2.bias', 'language_model.model.decoder.layers.4.fc2.weight', 'language_model.model.decoder.layers.4.final_layer_norm.bias', 'language_model.model.decoder.layers.4.final_layer_norm.weight', 'language_model.model.decoder.layers.4.self_attn.k_proj.bias', 'language_model.model.decoder.layers.4.self_attn.k_proj.weight', 'language_model.model.decoder.layers.4.self_attn.out_proj.bias', 'language_model.model.decoder.layers.4.self_attn.out_proj.weight', 'language_model.model.decoder.layers.4.self_attn.q_proj.bias', 'language_model.model.decoder.layers.4.self_attn.q_proj.weight', 'language_model.model.decoder.layers.4.self_attn.v_proj.bias', 'language_model.model.decoder.layers.4.self_attn.v_proj.weight', 'language_model.model.decoder.layers.4.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.4.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.encoder_attn.k_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.k_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.out_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.out_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.q_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.q_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn.v_proj.bias', 'language_model.model.decoder.layers.5.encoder_attn.v_proj.weight', 'language_model.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'language_model.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.fc1.bias', 'language_model.model.decoder.layers.5.fc1.weight', 'language_model.model.decoder.layers.5.fc2.bias', 'language_model.model.decoder.layers.5.fc2.weight', 'language_model.model.decoder.layers.5.final_layer_norm.bias', 'language_model.model.decoder.layers.5.final_layer_norm.weight', 'language_model.model.decoder.layers.5.self_attn.k_proj.bias', 'language_model.model.decoder.layers.5.self_attn.k_proj.weight', 'language_model.model.decoder.layers.5.self_attn.out_proj.bias', 'language_model.model.decoder.layers.5.self_attn.out_proj.weight', 'language_model.model.decoder.layers.5.self_attn.q_proj.bias', 'language_model.model.decoder.layers.5.self_attn.q_proj.weight', 'language_model.model.decoder.layers.5.self_attn.v_proj.bias', 'language_model.model.decoder.layers.5.self_attn.v_proj.weight', 'language_model.model.decoder.layers.5.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.5.self_attn_layer_norm.weight', 'language_model.model.encoder.embed_positions.weight', 'language_model.model.encoder.layernorm_embedding.bias', 'language_model.model.encoder.layernorm_embedding.weight', 'language_model.model.encoder.layers.0.fc1.bias', 'language_model.model.encoder.layers.0.fc1.weight', 'language_model.model.encoder.layers.0.fc2.bias', 'language_model.model.encoder.layers.0.fc2.weight', 'language_model.model.encoder.layers.0.final_layer_norm.bias', 'language_model.model.encoder.layers.0.final_layer_norm.weight', 'language_model.model.encoder.layers.0.self_attn.k_proj.bias', 'language_model.model.encoder.layers.0.self_attn.k_proj.weight', 'language_model.model.encoder.layers.0.self_attn.out_proj.bias', 'language_model.model.encoder.layers.0.self_attn.out_proj.weight', 'language_model.model.encoder.layers.0.self_attn.q_proj.bias', 'language_model.model.encoder.layers.0.self_attn.q_proj.weight', 'language_model.model.encoder.layers.0.self_attn.v_proj.bias', 'language_model.model.encoder.layers.0.self_attn.v_proj.weight', 'language_model.model.encoder.layers.0.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.0.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.1.fc1.bias', 'language_model.model.encoder.layers.1.fc1.weight', 'language_model.model.encoder.layers.1.fc2.bias', 'language_model.model.encoder.layers.1.fc2.weight', 'language_model.model.encoder.layers.1.final_layer_norm.bias', 'language_model.model.encoder.layers.1.final_layer_norm.weight', 'language_model.model.encoder.layers.1.self_attn.k_proj.bias', 'language_model.model.encoder.layers.1.self_attn.k_proj.weight', 'language_model.model.encoder.layers.1.self_attn.out_proj.bias', 'language_model.model.encoder.layers.1.self_attn.out_proj.weight', 'language_model.model.encoder.layers.1.self_attn.q_proj.bias', 'language_model.model.encoder.layers.1.self_attn.q_proj.weight', 'language_model.model.encoder.layers.1.self_attn.v_proj.bias', 'language_model.model.encoder.layers.1.self_attn.v_proj.weight', 'language_model.model.encoder.layers.1.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.1.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.2.fc1.bias', 'language_model.model.encoder.layers.2.fc1.weight', 'language_model.model.encoder.layers.2.fc2.bias', 'language_model.model.encoder.layers.2.fc2.weight', 'language_model.model.encoder.layers.2.final_layer_norm.bias', 'language_model.model.encoder.layers.2.final_layer_norm.weight', 'language_model.model.encoder.layers.2.self_attn.k_proj.bias', 'language_model.model.encoder.layers.2.self_attn.k_proj.weight', 'language_model.model.encoder.layers.2.self_attn.out_proj.bias', 'language_model.model.encoder.layers.2.self_attn.out_proj.weight', 'language_model.model.encoder.layers.2.self_attn.q_proj.bias', 'language_model.model.encoder.layers.2.self_attn.q_proj.weight', 'language_model.model.encoder.layers.2.self_attn.v_proj.bias', 'language_model.model.encoder.layers.2.self_attn.v_proj.weight', 'language_model.model.encoder.layers.2.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.2.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.3.fc1.bias', 'language_model.model.encoder.layers.3.fc1.weight', 'language_model.model.encoder.layers.3.fc2.bias', 'language_model.model.encoder.layers.3.fc2.weight', 'language_model.model.encoder.layers.3.final_layer_norm.bias', 'language_model.model.encoder.layers.3.final_layer_norm.weight', 'language_model.model.encoder.layers.3.self_attn.k_proj.bias', 'language_model.model.encoder.layers.3.self_attn.k_proj.weight', 'language_model.model.encoder.layers.3.self_attn.out_proj.bias', 'language_model.model.encoder.layers.3.self_attn.out_proj.weight', 'language_model.model.encoder.layers.3.self_attn.q_proj.bias', 'language_model.model.encoder.layers.3.self_attn.q_proj.weight', 'language_model.model.encoder.layers.3.self_attn.v_proj.bias', 'language_model.model.encoder.layers.3.self_attn.v_proj.weight', 'language_model.model.encoder.layers.3.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.3.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.4.fc1.bias', 'language_model.model.encoder.layers.4.fc1.weight', 'language_model.model.encoder.layers.4.fc2.bias', 'language_model.model.encoder.layers.4.fc2.weight', 'language_model.model.encoder.layers.4.final_layer_norm.bias', 'language_model.model.encoder.layers.4.final_layer_norm.weight', 'language_model.model.encoder.layers.4.self_attn.k_proj.bias', 'language_model.model.encoder.layers.4.self_attn.k_proj.weight', 'language_model.model.encoder.layers.4.self_attn.out_proj.bias', 'language_model.model.encoder.layers.4.self_attn.out_proj.weight', 'language_model.model.encoder.layers.4.self_attn.q_proj.bias', 'language_model.model.encoder.layers.4.self_attn.q_proj.weight', 'language_model.model.encoder.layers.4.self_attn.v_proj.bias', 'language_model.model.encoder.layers.4.self_attn.v_proj.weight', 'language_model.model.encoder.layers.4.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.4.self_attn_layer_norm.weight', 'language_model.model.encoder.layers.5.fc1.bias', 'language_model.model.encoder.layers.5.fc1.weight', 'language_model.model.encoder.layers.5.fc2.bias', 'language_model.model.encoder.layers.5.fc2.weight', 'language_model.model.encoder.layers.5.final_layer_norm.bias', 'language_model.model.encoder.layers.5.final_layer_norm.weight', 'language_model.model.encoder.layers.5.self_attn.k_proj.bias', 'language_model.model.encoder.layers.5.self_attn.k_proj.weight', 'language_model.model.encoder.layers.5.self_attn.out_proj.bias', 'language_model.model.encoder.layers.5.self_attn.out_proj.weight', 'language_model.model.encoder.layers.5.self_attn.q_proj.bias', 'language_model.model.encoder.layers.5.self_attn.q_proj.weight', 'language_model.model.encoder.layers.5.self_attn.v_proj.bias', 'language_model.model.encoder.layers.5.self_attn.v_proj.weight', 'language_model.model.encoder.layers.5.self_attn_layer_norm.bias', 'language_model.model.encoder.layers.5.self_attn_layer_norm.weight', 'language_model.model.shared.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.0.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.0.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.0.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.0.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.0.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.0.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.0.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.0.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.0.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.0.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.0.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.0.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.1.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.1.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.1.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.1.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.1.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.1.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.1.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.1.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.1.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.1.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.1.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.1.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.0.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.1.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.1.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.1.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.1.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.1.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.1.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.1.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.1.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.1.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.1.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.1.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.1.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.2.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.2.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.2.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.2.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.2.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.2.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.2.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.2.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.2.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.2.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.2.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.2.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.3.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.3.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.3.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.3.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.3.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.3.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.3.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.3.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.3.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.3.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.3.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.3.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.4.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.4.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.4.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.4.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.4.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.4.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.4.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.4.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.4.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.4.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.4.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.4.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.5.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.5.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.5.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.5.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.5.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.5.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.5.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.5.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.5.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.5.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.5.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.5.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.6.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.6.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.6.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.6.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.6.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.6.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.6.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.6.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.6.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.6.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.6.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.6.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.7.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.7.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.7.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.7.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.7.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.7.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.7.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.7.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.7.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.7.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.7.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.7.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.2.8.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.2.8.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.2.8.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.8.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.8.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.8.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.8.channel_block.ffn.norm.bias', 'vision_tower.blocks.2.8.channel_block.ffn.norm.weight', 'vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.2.8.spatial_block.ffn.norm.bias', 'vision_tower.blocks.2.8.spatial_block.ffn.norm.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.2.8.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.2.8.spatial_block.window_attn.norm.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.weight', 'vision_tower.blocks.3.0.channel_block.channel_attn.norm.bias', 'vision_tower.blocks.3.0.channel_block.channel_attn.norm.weight', 'vision_tower.blocks.3.0.channel_block.conv1.fn.dw.bias', 'vision_tower.blocks.3.0.channel_block.conv1.fn.dw.weight', 'vision_tower.blocks.3.0.channel_block.conv2.fn.dw.bias', 'vision_tower.blocks.3.0.channel_block.conv2.fn.dw.weight', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.3.0.channel_block.ffn.norm.bias', 'vision_tower.blocks.3.0.channel_block.ffn.norm.weight', 'vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.bias', 'vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.weight', 'vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.bias', 'vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.weight', 'vision_tower.blocks.3.0.spatial_block.ffn.norm.bias', 'vision_tower.blocks.3.0.spatial_block.ffn.norm.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.weight', 'vision_tower.blocks.3.0.spatial_block.window_attn.norm.bias', 'vision_tower.blocks.3.0.spatial_block.window_attn.norm.weight', 'vision_tower.convs.0.norm.bias', 'vision_tower.convs.0.norm.weight', 'vision_tower.convs.0.proj.bias', 'vision_tower.convs.0.proj.weight', 'vision_tower.convs.1.norm.bias', 'vision_tower.convs.1.norm.weight', 'vision_tower.convs.1.proj.bias', 'vision_tower.convs.1.proj.weight', 'vision_tower.convs.2.norm.bias', 'vision_tower.convs.2.norm.weight', 'vision_tower.convs.2.proj.bias', 'vision_tower.convs.2.proj.weight', 'vision_tower.convs.3.norm.bias', 'vision_tower.convs.3.norm.weight', 'vision_tower.convs.3.proj.bias', 'vision_tower.convs.3.proj.weight', 'visual_temporal_embed.pos_idx_to_embed']\n",
            "- This IS expected if you are initializing Florence2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Florence2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Florence2ForConditionalGeneration were not initialized from the model checkpoint at microsoft/Florence-2-base and are newly initialized: ['lm_head.weight', 'model.language_model.decoder.embed_positions.weight', 'model.language_model.decoder.embed_tokens.weight', 'model.language_model.decoder.layernorm_embedding.bias', 'model.language_model.decoder.layernorm_embedding.weight', 'model.language_model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.0.fc1.bias', 'model.language_model.decoder.layers.0.fc1.weight', 'model.language_model.decoder.layers.0.fc2.bias', 'model.language_model.decoder.layers.0.fc2.weight', 'model.language_model.decoder.layers.0.final_layer_norm.bias', 'model.language_model.decoder.layers.0.final_layer_norm.weight', 'model.language_model.decoder.layers.0.self_attn.k_proj.bias', 'model.language_model.decoder.layers.0.self_attn.k_proj.weight', 'model.language_model.decoder.layers.0.self_attn.out_proj.bias', 'model.language_model.decoder.layers.0.self_attn.out_proj.weight', 'model.language_model.decoder.layers.0.self_attn.q_proj.bias', 'model.language_model.decoder.layers.0.self_attn.q_proj.weight', 'model.language_model.decoder.layers.0.self_attn.v_proj.bias', 'model.language_model.decoder.layers.0.self_attn.v_proj.weight', 'model.language_model.decoder.layers.0.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.0.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.1.fc1.bias', 'model.language_model.decoder.layers.1.fc1.weight', 'model.language_model.decoder.layers.1.fc2.bias', 'model.language_model.decoder.layers.1.fc2.weight', 'model.language_model.decoder.layers.1.final_layer_norm.bias', 'model.language_model.decoder.layers.1.final_layer_norm.weight', 'model.language_model.decoder.layers.1.self_attn.k_proj.bias', 'model.language_model.decoder.layers.1.self_attn.k_proj.weight', 'model.language_model.decoder.layers.1.self_attn.out_proj.bias', 'model.language_model.decoder.layers.1.self_attn.out_proj.weight', 'model.language_model.decoder.layers.1.self_attn.q_proj.bias', 'model.language_model.decoder.layers.1.self_attn.q_proj.weight', 'model.language_model.decoder.layers.1.self_attn.v_proj.bias', 'model.language_model.decoder.layers.1.self_attn.v_proj.weight', 'model.language_model.decoder.layers.1.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.1.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.2.fc1.bias', 'model.language_model.decoder.layers.2.fc1.weight', 'model.language_model.decoder.layers.2.fc2.bias', 'model.language_model.decoder.layers.2.fc2.weight', 'model.language_model.decoder.layers.2.final_layer_norm.bias', 'model.language_model.decoder.layers.2.final_layer_norm.weight', 'model.language_model.decoder.layers.2.self_attn.k_proj.bias', 'model.language_model.decoder.layers.2.self_attn.k_proj.weight', 'model.language_model.decoder.layers.2.self_attn.out_proj.bias', 'model.language_model.decoder.layers.2.self_attn.out_proj.weight', 'model.language_model.decoder.layers.2.self_attn.q_proj.bias', 'model.language_model.decoder.layers.2.self_attn.q_proj.weight', 'model.language_model.decoder.layers.2.self_attn.v_proj.bias', 'model.language_model.decoder.layers.2.self_attn.v_proj.weight', 'model.language_model.decoder.layers.2.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.2.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.3.fc1.bias', 'model.language_model.decoder.layers.3.fc1.weight', 'model.language_model.decoder.layers.3.fc2.bias', 'model.language_model.decoder.layers.3.fc2.weight', 'model.language_model.decoder.layers.3.final_layer_norm.bias', 'model.language_model.decoder.layers.3.final_layer_norm.weight', 'model.language_model.decoder.layers.3.self_attn.k_proj.bias', 'model.language_model.decoder.layers.3.self_attn.k_proj.weight', 'model.language_model.decoder.layers.3.self_attn.out_proj.bias', 'model.language_model.decoder.layers.3.self_attn.out_proj.weight', 'model.language_model.decoder.layers.3.self_attn.q_proj.bias', 'model.language_model.decoder.layers.3.self_attn.q_proj.weight', 'model.language_model.decoder.layers.3.self_attn.v_proj.bias', 'model.language_model.decoder.layers.3.self_attn.v_proj.weight', 'model.language_model.decoder.layers.3.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.3.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.4.fc1.bias', 'model.language_model.decoder.layers.4.fc1.weight', 'model.language_model.decoder.layers.4.fc2.bias', 'model.language_model.decoder.layers.4.fc2.weight', 'model.language_model.decoder.layers.4.final_layer_norm.bias', 'model.language_model.decoder.layers.4.final_layer_norm.weight', 'model.language_model.decoder.layers.4.self_attn.k_proj.bias', 'model.language_model.decoder.layers.4.self_attn.k_proj.weight', 'model.language_model.decoder.layers.4.self_attn.out_proj.bias', 'model.language_model.decoder.layers.4.self_attn.out_proj.weight', 'model.language_model.decoder.layers.4.self_attn.q_proj.bias', 'model.language_model.decoder.layers.4.self_attn.q_proj.weight', 'model.language_model.decoder.layers.4.self_attn.v_proj.bias', 'model.language_model.decoder.layers.4.self_attn.v_proj.weight', 'model.language_model.decoder.layers.4.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.4.self_attn_layer_norm.weight', 'model.language_model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.language_model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.language_model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.language_model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.language_model.decoder.layers.5.fc1.bias', 'model.language_model.decoder.layers.5.fc1.weight', 'model.language_model.decoder.layers.5.fc2.bias', 'model.language_model.decoder.layers.5.fc2.weight', 'model.language_model.decoder.layers.5.final_layer_norm.bias', 'model.language_model.decoder.layers.5.final_layer_norm.weight', 'model.language_model.decoder.layers.5.self_attn.k_proj.bias', 'model.language_model.decoder.layers.5.self_attn.k_proj.weight', 'model.language_model.decoder.layers.5.self_attn.out_proj.bias', 'model.language_model.decoder.layers.5.self_attn.out_proj.weight', 'model.language_model.decoder.layers.5.self_attn.q_proj.bias', 'model.language_model.decoder.layers.5.self_attn.q_proj.weight', 'model.language_model.decoder.layers.5.self_attn.v_proj.bias', 'model.language_model.decoder.layers.5.self_attn.v_proj.weight', 'model.language_model.decoder.layers.5.self_attn_layer_norm.bias', 'model.language_model.decoder.layers.5.self_attn_layer_norm.weight', 'model.language_model.encoder.embed_positions.weight', 'model.language_model.encoder.embed_tokens.weight', 'model.language_model.encoder.layernorm_embedding.bias', 'model.language_model.encoder.layernorm_embedding.weight', 'model.language_model.encoder.layers.0.fc1.bias', 'model.language_model.encoder.layers.0.fc1.weight', 'model.language_model.encoder.layers.0.fc2.bias', 'model.language_model.encoder.layers.0.fc2.weight', 'model.language_model.encoder.layers.0.final_layer_norm.bias', 'model.language_model.encoder.layers.0.final_layer_norm.weight', 'model.language_model.encoder.layers.0.self_attn.k_proj.bias', 'model.language_model.encoder.layers.0.self_attn.k_proj.weight', 'model.language_model.encoder.layers.0.self_attn.out_proj.bias', 'model.language_model.encoder.layers.0.self_attn.out_proj.weight', 'model.language_model.encoder.layers.0.self_attn.q_proj.bias', 'model.language_model.encoder.layers.0.self_attn.q_proj.weight', 'model.language_model.encoder.layers.0.self_attn.v_proj.bias', 'model.language_model.encoder.layers.0.self_attn.v_proj.weight', 'model.language_model.encoder.layers.0.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.0.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.1.fc1.bias', 'model.language_model.encoder.layers.1.fc1.weight', 'model.language_model.encoder.layers.1.fc2.bias', 'model.language_model.encoder.layers.1.fc2.weight', 'model.language_model.encoder.layers.1.final_layer_norm.bias', 'model.language_model.encoder.layers.1.final_layer_norm.weight', 'model.language_model.encoder.layers.1.self_attn.k_proj.bias', 'model.language_model.encoder.layers.1.self_attn.k_proj.weight', 'model.language_model.encoder.layers.1.self_attn.out_proj.bias', 'model.language_model.encoder.layers.1.self_attn.out_proj.weight', 'model.language_model.encoder.layers.1.self_attn.q_proj.bias', 'model.language_model.encoder.layers.1.self_attn.q_proj.weight', 'model.language_model.encoder.layers.1.self_attn.v_proj.bias', 'model.language_model.encoder.layers.1.self_attn.v_proj.weight', 'model.language_model.encoder.layers.1.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.1.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.2.fc1.bias', 'model.language_model.encoder.layers.2.fc1.weight', 'model.language_model.encoder.layers.2.fc2.bias', 'model.language_model.encoder.layers.2.fc2.weight', 'model.language_model.encoder.layers.2.final_layer_norm.bias', 'model.language_model.encoder.layers.2.final_layer_norm.weight', 'model.language_model.encoder.layers.2.self_attn.k_proj.bias', 'model.language_model.encoder.layers.2.self_attn.k_proj.weight', 'model.language_model.encoder.layers.2.self_attn.out_proj.bias', 'model.language_model.encoder.layers.2.self_attn.out_proj.weight', 'model.language_model.encoder.layers.2.self_attn.q_proj.bias', 'model.language_model.encoder.layers.2.self_attn.q_proj.weight', 'model.language_model.encoder.layers.2.self_attn.v_proj.bias', 'model.language_model.encoder.layers.2.self_attn.v_proj.weight', 'model.language_model.encoder.layers.2.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.2.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.3.fc1.bias', 'model.language_model.encoder.layers.3.fc1.weight', 'model.language_model.encoder.layers.3.fc2.bias', 'model.language_model.encoder.layers.3.fc2.weight', 'model.language_model.encoder.layers.3.final_layer_norm.bias', 'model.language_model.encoder.layers.3.final_layer_norm.weight', 'model.language_model.encoder.layers.3.self_attn.k_proj.bias', 'model.language_model.encoder.layers.3.self_attn.k_proj.weight', 'model.language_model.encoder.layers.3.self_attn.out_proj.bias', 'model.language_model.encoder.layers.3.self_attn.out_proj.weight', 'model.language_model.encoder.layers.3.self_attn.q_proj.bias', 'model.language_model.encoder.layers.3.self_attn.q_proj.weight', 'model.language_model.encoder.layers.3.self_attn.v_proj.bias', 'model.language_model.encoder.layers.3.self_attn.v_proj.weight', 'model.language_model.encoder.layers.3.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.3.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.4.fc1.bias', 'model.language_model.encoder.layers.4.fc1.weight', 'model.language_model.encoder.layers.4.fc2.bias', 'model.language_model.encoder.layers.4.fc2.weight', 'model.language_model.encoder.layers.4.final_layer_norm.bias', 'model.language_model.encoder.layers.4.final_layer_norm.weight', 'model.language_model.encoder.layers.4.self_attn.k_proj.bias', 'model.language_model.encoder.layers.4.self_attn.k_proj.weight', 'model.language_model.encoder.layers.4.self_attn.out_proj.bias', 'model.language_model.encoder.layers.4.self_attn.out_proj.weight', 'model.language_model.encoder.layers.4.self_attn.q_proj.bias', 'model.language_model.encoder.layers.4.self_attn.q_proj.weight', 'model.language_model.encoder.layers.4.self_attn.v_proj.bias', 'model.language_model.encoder.layers.4.self_attn.v_proj.weight', 'model.language_model.encoder.layers.4.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.4.self_attn_layer_norm.weight', 'model.language_model.encoder.layers.5.fc1.bias', 'model.language_model.encoder.layers.5.fc1.weight', 'model.language_model.encoder.layers.5.fc2.bias', 'model.language_model.encoder.layers.5.fc2.weight', 'model.language_model.encoder.layers.5.final_layer_norm.bias', 'model.language_model.encoder.layers.5.final_layer_norm.weight', 'model.language_model.encoder.layers.5.self_attn.k_proj.bias', 'model.language_model.encoder.layers.5.self_attn.k_proj.weight', 'model.language_model.encoder.layers.5.self_attn.out_proj.bias', 'model.language_model.encoder.layers.5.self_attn.out_proj.weight', 'model.language_model.encoder.layers.5.self_attn.q_proj.bias', 'model.language_model.encoder.layers.5.self_attn.q_proj.weight', 'model.language_model.encoder.layers.5.self_attn.v_proj.bias', 'model.language_model.encoder.layers.5.self_attn.v_proj.weight', 'model.language_model.encoder.layers.5.self_attn_layer_norm.bias', 'model.language_model.encoder.layers.5.self_attn_layer_norm.weight', 'model.language_model.shared.weight', 'model.multi_modal_projector.image_position_embed.column_embeddings.weight', 'model.multi_modal_projector.image_position_embed.row_embeddings.weight', 'model.multi_modal_projector.image_proj_norm.bias', 'model.multi_modal_projector.image_proj_norm.weight', 'model.multi_modal_projector.image_projection.weight', 'model.multi_modal_projector.visual_temporal_embed.pos_idx_to_embed', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.0.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.0.0.channel_block.conv1.bias', 'model.vision_tower.blocks.0.0.channel_block.conv1.weight', 'model.vision_tower.blocks.0.0.channel_block.conv2.bias', 'model.vision_tower.blocks.0.0.channel_block.conv2.weight', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.0.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.0.0.channel_block.norm1.bias', 'model.vision_tower.blocks.0.0.channel_block.norm1.weight', 'model.vision_tower.blocks.0.0.channel_block.norm2.bias', 'model.vision_tower.blocks.0.0.channel_block.norm2.weight', 'model.vision_tower.blocks.0.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.0.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.0.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.0.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.0.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.0.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.0.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.0.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.0.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.0.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.1.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.1.0.channel_block.conv1.bias', 'model.vision_tower.blocks.1.0.channel_block.conv1.weight', 'model.vision_tower.blocks.1.0.channel_block.conv2.bias', 'model.vision_tower.blocks.1.0.channel_block.conv2.weight', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.1.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.1.0.channel_block.norm1.bias', 'model.vision_tower.blocks.1.0.channel_block.norm1.weight', 'model.vision_tower.blocks.1.0.channel_block.norm2.bias', 'model.vision_tower.blocks.1.0.channel_block.norm2.weight', 'model.vision_tower.blocks.1.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.1.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.1.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.1.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.1.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.1.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.1.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.1.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.1.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.1.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.0.channel_block.conv1.bias', 'model.vision_tower.blocks.2.0.channel_block.conv1.weight', 'model.vision_tower.blocks.2.0.channel_block.conv2.bias', 'model.vision_tower.blocks.2.0.channel_block.conv2.weight', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.0.channel_block.norm1.bias', 'model.vision_tower.blocks.2.0.channel_block.norm1.weight', 'model.vision_tower.blocks.2.0.channel_block.norm2.bias', 'model.vision_tower.blocks.2.0.channel_block.norm2.weight', 'model.vision_tower.blocks.2.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.1.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.1.channel_block.conv1.bias', 'model.vision_tower.blocks.2.1.channel_block.conv1.weight', 'model.vision_tower.blocks.2.1.channel_block.conv2.bias', 'model.vision_tower.blocks.2.1.channel_block.conv2.weight', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.1.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.1.channel_block.norm1.bias', 'model.vision_tower.blocks.2.1.channel_block.norm1.weight', 'model.vision_tower.blocks.2.1.channel_block.norm2.bias', 'model.vision_tower.blocks.2.1.channel_block.norm2.weight', 'model.vision_tower.blocks.2.1.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.1.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.1.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.1.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.1.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.1.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.1.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.1.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.1.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.1.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.2.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.2.channel_block.conv1.bias', 'model.vision_tower.blocks.2.2.channel_block.conv1.weight', 'model.vision_tower.blocks.2.2.channel_block.conv2.bias', 'model.vision_tower.blocks.2.2.channel_block.conv2.weight', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.2.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.2.channel_block.norm1.bias', 'model.vision_tower.blocks.2.2.channel_block.norm1.weight', 'model.vision_tower.blocks.2.2.channel_block.norm2.bias', 'model.vision_tower.blocks.2.2.channel_block.norm2.weight', 'model.vision_tower.blocks.2.2.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.2.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.2.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.2.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.2.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.2.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.2.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.2.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.2.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.2.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.3.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.3.channel_block.conv1.bias', 'model.vision_tower.blocks.2.3.channel_block.conv1.weight', 'model.vision_tower.blocks.2.3.channel_block.conv2.bias', 'model.vision_tower.blocks.2.3.channel_block.conv2.weight', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.3.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.3.channel_block.norm1.bias', 'model.vision_tower.blocks.2.3.channel_block.norm1.weight', 'model.vision_tower.blocks.2.3.channel_block.norm2.bias', 'model.vision_tower.blocks.2.3.channel_block.norm2.weight', 'model.vision_tower.blocks.2.3.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.3.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.3.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.3.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.3.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.3.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.3.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.3.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.3.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.3.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.4.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.4.channel_block.conv1.bias', 'model.vision_tower.blocks.2.4.channel_block.conv1.weight', 'model.vision_tower.blocks.2.4.channel_block.conv2.bias', 'model.vision_tower.blocks.2.4.channel_block.conv2.weight', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.4.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.4.channel_block.norm1.bias', 'model.vision_tower.blocks.2.4.channel_block.norm1.weight', 'model.vision_tower.blocks.2.4.channel_block.norm2.bias', 'model.vision_tower.blocks.2.4.channel_block.norm2.weight', 'model.vision_tower.blocks.2.4.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.4.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.4.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.4.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.4.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.4.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.4.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.4.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.4.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.4.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.5.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.5.channel_block.conv1.bias', 'model.vision_tower.blocks.2.5.channel_block.conv1.weight', 'model.vision_tower.blocks.2.5.channel_block.conv2.bias', 'model.vision_tower.blocks.2.5.channel_block.conv2.weight', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.5.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.5.channel_block.norm1.bias', 'model.vision_tower.blocks.2.5.channel_block.norm1.weight', 'model.vision_tower.blocks.2.5.channel_block.norm2.bias', 'model.vision_tower.blocks.2.5.channel_block.norm2.weight', 'model.vision_tower.blocks.2.5.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.5.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.5.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.5.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.5.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.5.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.5.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.5.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.5.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.5.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.6.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.6.channel_block.conv1.bias', 'model.vision_tower.blocks.2.6.channel_block.conv1.weight', 'model.vision_tower.blocks.2.6.channel_block.conv2.bias', 'model.vision_tower.blocks.2.6.channel_block.conv2.weight', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.6.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.6.channel_block.norm1.bias', 'model.vision_tower.blocks.2.6.channel_block.norm1.weight', 'model.vision_tower.blocks.2.6.channel_block.norm2.bias', 'model.vision_tower.blocks.2.6.channel_block.norm2.weight', 'model.vision_tower.blocks.2.6.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.6.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.6.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.6.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.6.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.6.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.6.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.6.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.6.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.6.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.7.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.7.channel_block.conv1.bias', 'model.vision_tower.blocks.2.7.channel_block.conv1.weight', 'model.vision_tower.blocks.2.7.channel_block.conv2.bias', 'model.vision_tower.blocks.2.7.channel_block.conv2.weight', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.7.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.7.channel_block.norm1.bias', 'model.vision_tower.blocks.2.7.channel_block.norm1.weight', 'model.vision_tower.blocks.2.7.channel_block.norm2.bias', 'model.vision_tower.blocks.2.7.channel_block.norm2.weight', 'model.vision_tower.blocks.2.7.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.7.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.7.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.7.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.7.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.7.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.7.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.7.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.7.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.7.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.2.8.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.2.8.channel_block.conv1.bias', 'model.vision_tower.blocks.2.8.channel_block.conv1.weight', 'model.vision_tower.blocks.2.8.channel_block.conv2.bias', 'model.vision_tower.blocks.2.8.channel_block.conv2.weight', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.8.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.8.channel_block.norm1.bias', 'model.vision_tower.blocks.2.8.channel_block.norm1.weight', 'model.vision_tower.blocks.2.8.channel_block.norm2.bias', 'model.vision_tower.blocks.2.8.channel_block.norm2.weight', 'model.vision_tower.blocks.2.8.spatial_block.conv1.bias', 'model.vision_tower.blocks.2.8.spatial_block.conv1.weight', 'model.vision_tower.blocks.2.8.spatial_block.conv2.bias', 'model.vision_tower.blocks.2.8.spatial_block.conv2.weight', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.2.8.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.2.8.spatial_block.norm1.bias', 'model.vision_tower.blocks.2.8.spatial_block.norm1.weight', 'model.vision_tower.blocks.2.8.spatial_block.norm2.bias', 'model.vision_tower.blocks.2.8.spatial_block.norm2.weight', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.2.8.spatial_block.window_attn.qkv.weight', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.proj.bias', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.proj.weight', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.qkv.bias', 'model.vision_tower.blocks.3.0.channel_block.channel_attn.qkv.weight', 'model.vision_tower.blocks.3.0.channel_block.conv1.bias', 'model.vision_tower.blocks.3.0.channel_block.conv1.weight', 'model.vision_tower.blocks.3.0.channel_block.conv2.bias', 'model.vision_tower.blocks.3.0.channel_block.conv2.weight', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc1.bias', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc1.weight', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc2.bias', 'model.vision_tower.blocks.3.0.channel_block.ffn.fc2.weight', 'model.vision_tower.blocks.3.0.channel_block.norm1.bias', 'model.vision_tower.blocks.3.0.channel_block.norm1.weight', 'model.vision_tower.blocks.3.0.channel_block.norm2.bias', 'model.vision_tower.blocks.3.0.channel_block.norm2.weight', 'model.vision_tower.blocks.3.0.spatial_block.conv1.bias', 'model.vision_tower.blocks.3.0.spatial_block.conv1.weight', 'model.vision_tower.blocks.3.0.spatial_block.conv2.bias', 'model.vision_tower.blocks.3.0.spatial_block.conv2.weight', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc1.bias', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc1.weight', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc2.bias', 'model.vision_tower.blocks.3.0.spatial_block.ffn.fc2.weight', 'model.vision_tower.blocks.3.0.spatial_block.norm1.bias', 'model.vision_tower.blocks.3.0.spatial_block.norm1.weight', 'model.vision_tower.blocks.3.0.spatial_block.norm2.bias', 'model.vision_tower.blocks.3.0.spatial_block.norm2.weight', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.proj.bias', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.proj.weight', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.qkv.bias', 'model.vision_tower.blocks.3.0.spatial_block.window_attn.qkv.weight', 'model.vision_tower.convs.0.conv.bias', 'model.vision_tower.convs.0.conv.weight', 'model.vision_tower.convs.0.norm.bias', 'model.vision_tower.convs.0.norm.weight', 'model.vision_tower.convs.1.conv.bias', 'model.vision_tower.convs.1.conv.weight', 'model.vision_tower.convs.1.norm.bias', 'model.vision_tower.convs.1.norm.weight', 'model.vision_tower.convs.2.conv.bias', 'model.vision_tower.convs.2.conv.weight', 'model.vision_tower.convs.2.norm.bias', 'model.vision_tower.convs.2.norm.weight', 'model.vision_tower.convs.3.conv.bias', 'model.vision_tower.convs.3.conv.weight', 'model.vision_tower.convs.3.norm.bias', 'model.vision_tower.convs.3.norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Image features and image tokens do not match: tokens: 0, features 577",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3486134536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m ).to(model.device)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m             special_image_mask = self.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1037\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     ):\n\u001b[0;32m-> 1015\u001b[0;31m         return self.model.get_placeholder_mask(\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/florence2/modeling_florence2.py\u001b[0m in \u001b[0;36mget_placeholder_mask\u001b[0;34m(self, input_ids, inputs_embeds, image_features)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mn_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecial_image_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    708\u001b[0m                 \u001b[0;34mf\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Image features and image tokens do not match: tokens: 0, features 577"
          ]
        }
      ]
    }
  ]
}