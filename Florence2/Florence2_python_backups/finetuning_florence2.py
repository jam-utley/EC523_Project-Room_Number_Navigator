# -*- coding: utf-8 -*-
"""Finetuning_Florence2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUrmJKuXK4GX2Y41T2MJldH9KRV1PS5T

[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)

# Fine-tuning Florence-2 on Object Detection Dataset

---

[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb)
[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/florence-2/)
[![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)

This google notebook is an adaptation of the one found at the links above, published through Roboflow. I have removed the parts of the code unnecessary to our project.

In order to be reproducable, this notebook requires Roboflow and HuggingFace API Tokens.

## Downloading necessary libraries
"""

!pip install bitsandbytes transformers accelerate peft datasets safetensors roboflow supervision

# @title Imports

import io
import os
import re
import json
import torch
import html
import base64
import itertools

import numpy as np
import supervision as sv

from google.colab import userdata
from IPython.core.display import display, HTML
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    get_scheduler
)
from torch.optim import AdamW
from tqdm import tqdm
from typing import List, Dict, Any, Tuple, Generator
from peft import LoraConfig, get_peft_model
from PIL import Image
from roboflow import Roboflow



#My stuff. May have redundancies
import torch
from transformers import (
    AutoProcessor,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainerCallback,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
)
import matplotlib.pyplot as plt

"""Load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor` classes from the transformers library. Note that you need to pass `trust_remote_code` as `True` since this model is not a standard transformers model."""

from google.colab import drive
drive.mount('/content/drive')

#workaround for unnecessary flash_attn requirement
#workaround attribute to James Braza https://huggingface.co/jamesbraza

CHECKPOINT = "microsoft/Florence-2-base-ft"
REVISION = 'refs/pr/6'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from unittest.mock import patch
from transformers.dynamic_module_utils import get_imports

from transformers import AutoProcessor, AutoModelForCausalLM
import torch

import os

model_path = "microsoft/Florence-2-base"

def fixed_get_imports(filename: str | os.PathLike) -> list[str]:
    if not str(filename).endswith("modeling_florence2.py"):
        return get_imports(filename)
    imports = get_imports(filename)
    if "flash_attn" in imports:
      imports.remove("flash_attn")
    return imports

with patch("transformers.dynamic_module_utils.get_imports", fixed_get_imports): #workaround for unnecessary flash_attn requirement
    model = AutoModelForCausalLM.from_pretrained(model_path,
                                                 attn_implementation="eager",
                                                 torch_dtype=torch.float32,
                                                 trust_remote_code=True,
                                                 force_download = True)
    processor = AutoProcessor.from_pretrained(
      model_path,
      trust_remote_code=True,
      force_download = True
    )
#end attribute to James Braza

model.image_projection.data = model.image_projection.data.to("cuda")

for name, param in model.named_parameters():
    if param.device.type != "cuda":
        print("CPU module:", name, param.device)
        break

#Sends all parts to CUDA!

DEVICE = torch.device("cuda")

#image = Image.open(EXAMPLE_IMAGE_PATH)
task = "<OD>"
text = "<OD>"

# Make sure inputs are on CUDA
#inputs = processor(
    #text=text,
    #images=image,
    #return_tensors="pt"
#).to(DEVICE)

# Florence 2 requires explicit moves of internal towers
model.to(DEVICE)

model.image_projection.data = model.image_projection.data.to("cuda") #Catches the last one hiding on the CPU

if hasattr(model, "vision_tower"):
    model.vision_tower.to(DEVICE)

if hasattr(model, "text_encoder"):
    model.text_encoder.to(DEVICE)

if hasattr(model, "decoder"):
    model.decoder.to(DEVICE)

if hasattr(model, "vae"):
    model.vae.to(DEVICE)

"""## Fine-tune Florence-2 on custom dataset

### Download dataset from Roboflow Universe
"""

ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
rf = Roboflow(api_key=ROBOFLOW_API_KEY)

project = rf.workspace("ec523-final-project").project("bounding-box-qfdtu")
version = project.version(6)
dataset = version.download("florence2-od") #This is not the name of the version, but it is the way it shall be configured

!head -n 5 {dataset.location}/train/annotations.jsonl

# @title Define `DetectionsDataset` class

class JSONLDataset:
    def __init__(self, jsonl_file_path: str, image_directory_path: str):
        self.jsonl_file_path = jsonl_file_path
        self.image_directory_path = image_directory_path
        self.entries = self._load_entries()

    def _load_entries(self) -> List[Dict[str, Any]]:
        entries = []
        with open(self.jsonl_file_path, 'r') as file:
            for line in file:
                data = json.loads(line)
                entries.append(data)
        return entries

    def __len__(self) -> int:
        return len(self.entries)

    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:
        if idx < 0 or idx >= len(self.entries):
            raise IndexError("Index out of range")

        entry = self.entries[idx]
        image_path = os.path.join(self.image_directory_path, entry['image'])
        try:
            image = Image.open(image_path)
            return (image, entry)
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file {image_path} not found.")


class DetectionDataset(Dataset):
    def __init__(self, jsonl_file_path: str, image_directory_path: str):
        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, data = self.dataset[idx]
        prefix = data['prefix']
        suffix = data['suffix']
        return prefix, suffix, image

# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets

BATCH_SIZE = 6
NUM_WORKERS = 0

def collate_fn(batch):
    questions, answers, images = zip(*batch)
    inputs = processor(text=list(questions), images=list(images), return_tensors="pt", padding=True).to(DEVICE)
    return inputs, answers

train_dataset = DetectionDataset(
    jsonl_file_path = f"{dataset.location}/train/annotations.jsonl",
    image_directory_path = f"{dataset.location}/train/"
)
val_dataset = DetectionDataset(
    jsonl_file_path = f"{dataset.location}/valid/annotations.jsonl",
    image_directory_path = f"{dataset.location}/valid/"
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)

# @title Setup LoRA Florence-2 model

config = LoraConfig(
    r=8,
    lora_alpha=8,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "linear", "Conv2d", "lm_head", "fc2"],
    task_type="CAUSAL_LM",
    lora_dropout=0.05,
    bias="none",
    inference_mode=False,
    use_rslora=True,
    init_lora_weights="gaussian",
    revision=REVISION
)

peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()

torch.cuda.empty_cache()

# @title Run inference with pre-trained Florence-2 model on validation dataset

def render_inline(image: Image.Image, resize=(128, 128)):
    """Convert image into inline html."""
    image.resize(resize)
    with io.BytesIO() as buffer:
        image.save(buffer, format='jpeg')
        image_b64 = str(base64.b64encode(buffer.getvalue()), "utf-8")
        return f"data:image/jpeg;base64,{image_b64}"


def render_example(image: Image.Image, response):
    try:
        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)
        #Only Search for Door Panels
        mask = [cls == "Door Panel" for cls in detections.class_name]
        detections = detections[mask]

        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)
        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)
    except:
        print('failed to render model response')
    return f"""
<div style="display: inline-flex; align-items: center; justify-content: center;">
    <img style="width:256px; height:256px;" src="{render_inline(image, resize=(128, 128))}" />
    <p style="width:512px; margin:10px; font-size:small;">{html.escape(json.dumps(response))}</p>
</div>
"""


def render_inference_results(model, dataset: DetectionDataset, count: int):
    html_out = ""
    count = min(count, len(dataset))
    for i in range(count):
        image, data = dataset.dataset[i]
        prefix = data['prefix']
        suffix = data['suffix']
        inputs = processor(text=prefix, images=image, return_tensors="pt").to(DEVICE)
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            num_beams=3,
            use_cache=False
        )
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        answer = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)
        html_out += render_example(image, answer)

    display(HTML(html_out))

render_inference_results(peft_model, val_dataset, 4)

"""## Fine-tune Florence-2 on custom object detection dataset"""

# @title Define train loop

def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6, loss_per_epoch=[]):
    optimizer = AdamW(model.parameters(), lr=lr)
    num_training_steps = epochs * len(train_loader)
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )

    render_inference_results(peft_model, val_loader.dataset, 6)

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_loss_per_epoch = []
        for inputs, answers in tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}"):

            input_ids = inputs["input_ids"]
            pixel_values = inputs["pixel_values"]
            labels = processor.tokenizer(
                text=answers,
                return_tensors="pt",
                padding=True,
                return_token_type_ids=False
            ).input_ids.to(DEVICE)

            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
            loss = outputs.loss

            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()

            train_loss_per_epoch.append(loss.item())
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        #loss_per_epoch.append(avg_train_loss)
        print(f"Average Training Loss: {avg_train_loss}")

        model.eval()
        val_loss = 0
        val_loss_per_epoch = []
        with torch.no_grad():
            for inputs, answers in tqdm(val_loader, desc=f"Validation Epoch {epoch + 1}/{epochs}"):

                input_ids = inputs["input_ids"]
                pixel_values = inputs["pixel_values"]
                labels = processor.tokenizer(
                    text=answers,
                    return_tensors="pt",
                    padding=True,
                    return_token_type_ids=False
                ).input_ids.to(DEVICE)

                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
                loss = outputs.loss

                val_loss += loss.item()
                val_loss_per_epoch.append(loss.item())

            avg_val_loss = val_loss / len(val_loader)
            #loss_per_epoch.append(avg_val_loss)
            print(f"Average Validation Loss: {avg_val_loss}")

            render_inference_results(peft_model, val_loader.dataset, 6)

        loss_per_epoch.append((avg_train_loss, avg_val_loss))

        output_dir = f"./model_checkpoints/epoch_{epoch+1}"
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        processor.save_pretrained(output_dir)
        plt.figure()
        plt.plot(val_loss_per_epoch, label='Validation Loss')
        plt.plot(train_loss_per_epoch, label='Training Loss')
        plt.xlabel('Batch')
        plt.ylabel('Loss')
        plt.title(f'Training and Validation Loss, Epoch: {epoch}')
        plt.legend()
        plt.savefig(f"train_val_{epoch}_{len(loss_per_epoch)}.png") #Adds loss_per_epoch to make sure it doesn't save over old versions and error out
        plt.close()
    return loss_per_epoch

# Commented out IPython magic to ensure Python compatibility.
# # @title Run train loop
# 
# %%time
# 
# EPOCHS = 10
# LR = 5e-6
# 
# loss_per_epoch = []
# for i in range(5):
#   LR = LR * 10 #increases learning rate each epoch
#   loss_per_epoch = train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR, loss_per_epoch=loss_per_epoch)

print(loss_per_epoch)

#10 epochs per run
#5 runs
num_runs = len(loss_per_epoch)/10
run1 = loss_per_epoch[:10]
run2 = loss_per_epoch[10:20]
run3 = loss_per_epoch[20:30]
run4 = loss_per_epoch[30:40]
run5 = loss_per_epoch[40:]
plt.plot(run1, label='5e-6')  #LR = LR * (0.8 ** i)
plt.plot(run2, label='5e-5')
plt.plot(run3, label='5e-4')
plt.plot(run4, label='5e-3')
plt.plot(run5, label='5e-2')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss per Epoch')
plt.legend()
plt.savefig("all_train_val_losses.png")

import os
from google.colab import files

folder = "/content"

# List files directly in the folder
for filename in os.listdir(folder):
    full_path = os.path.join(folder, filename)

    # skip directories
    if os.path.isdir(full_path):
        continue

    # only download PNGs
    if filename.lower().endswith(".png"):
        files.download(full_path)

!zip -r model_checkpoints.zip model_checkpoints

from google.colab import files
files.download("model_checkpoints.zip")

torch.save(model.state_dict(), "florence2_weights_all.pt")
model.save_pretrained("/content/florence2-lora", safe_serialization=False)
processor.save_pretrained("/content/florence2-lora/")

!zip -r florence2-LoRA.zip florence2-lora
!zip -r florence2-weights-all.zip florence2_weights_all.pt

from google.colab import files
files.download("florence2-LoRA.zip")
files.download("florence2-weights-all.zip")

peft_model.save_pretrained("/content/florence2-lora")
processor.save_pretrained("/content/florence2-lora/")
!ls -la /content/florence2-lora/