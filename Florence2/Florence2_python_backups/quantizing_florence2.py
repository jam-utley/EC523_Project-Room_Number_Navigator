# -*- coding: utf-8 -*-
"""Quantizing_Florence2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZnZP8AcO0IxxLdAQj462-64hP4cF5Biv

##INSTALL NECESSARY LIBRARIES AND VERSIONS
"""

!pip install bitsandbytes transformers==4.49 accelerate peft datasets safetensors

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers==4.49

!pip install autoawq

"""##DOWNLOAD MODEL"""

#Ok. This works. MAKE SURE TRANSFORMERS >=4.39, <=4.49!
#Use Transformers 4.49 for quantization. It works for both florence2 AND bitsandbytes

#workaround for unnecessary flash_attn requirement
#workaround attribute to James Braza https://huggingface.co/jamesbraza

from unittest.mock import patch
from transformers.dynamic_module_utils import get_imports

from transformers import AutoProcessor, AutoModelForCausalLM
import torch

import os

model_path = "microsoft/Florence-2-base"

def fixed_get_imports(filename: str | os.PathLike) -> list[str]:
    if not str(filename).endswith("modeling_florence2.py"):
        return get_imports(filename)
    imports = get_imports(filename)
    if "flash_attn" in imports:
      imports.remove("flash_attn")
    return imports

with patch("transformers.dynamic_module_utils.get_imports", fixed_get_imports): #workaround for unnecessary flash_attn requirement
    model = AutoModelForCausalLM.from_pretrained(model_path,
                                                 attn_implementation="sdpa",
                                                 torch_dtype=torch.float32,
                                                 trust_remote_code=True,
                                                 force_download = True)
    processor = AutoProcessor.from_pretrained(
      model_path,
      trust_remote_code=True,
      force_download = True
    )
#end attribute to James Braza

"""##QUANTIZATION"""

#Quantized with BitsandBytes
#bitsandbytes sucks this doesn't run right

#MAKE SURE TRANSFORMERS == 4.49!

#workaround for unnecessary flash_attn requirement
#workaround attribute to James Braza https://huggingface.co/jamesbraza

from unittest.mock import patch
from transformers.dynamic_module_utils import get_imports
from transformers import AutoProcessor, AutoModelForCausalLM
import torch, os

model_path = "microsoft/Florence-2-base"

def fixed_get_imports(filename: str | os.PathLike) -> list[str]:
    if not str(filename).endswith("modeling_florence2.py"):
        return get_imports(filename)
    imports = get_imports(filename)
    if "flash_attn" in imports:
        imports.remove("flash_attn")
    return imports

# 4bit quantization config
bnb_config = dict(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

with patch("transformers.dynamic_module_utils.get_imports", fixed_get_imports):
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        attn_implementation="sdpa",
        **bnb_config
    )
    processor = AutoProcessor.from_pretrained(
        model_path,
        trust_remote_code=True
    )

"""###SAVE QUANTIZATION"""

#SAVE QUANTIZATION
save_path = "/content/florence2-4bit-optimum"
model.save_pretrained(save_path)
processor.save_pretrained(save_path)

print("Saved to", save_path)

!zip -r florence2-4bit-bitsnbytes_v1.zip florence2-4bit-optimum

from google.colab import files

files.download('florence2-4bit-bitsnbytes_v1.zip')