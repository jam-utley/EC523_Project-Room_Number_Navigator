{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUxtdocsru1e"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjcSm8EZru1f"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers\n",
        "!pip install -q timm einops\n",
        "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available, using standard attention\"\n",
        "!pip install -q opencv-python pillow matplotlib\n",
        "!pip install -q scipy pandas\n",
        "\n",
        "print(\"\\n Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN7Mui5Hru1f"
      },
      "outputs": [],
      "source": [
        "# Clone YOLOv7 repository\n",
        "%cd /content\n",
        "!rm -rf yolov7\n",
        "!git clone https://github.com/WongKinYiu/yolov7.git\n",
        "%cd yolov7\n",
        "\n",
        "print(\"\\n YOLOv7 repository cloned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIvUXtVXru1f"
      },
      "outputs": [],
      "source": [
        "# Fix torch.load for newer PyTorch versions\n",
        "import os\n",
        "import re\n",
        "\n",
        "def fix_torch_load(filepath):\n",
        "    \"\"\"Add weights_only=False to torch.load calls\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        return False\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    original = content\n",
        "\n",
        "    # Pattern to find torch.load without weights_only\n",
        "    patterns = [\n",
        "        (r\"torch\\.load\\(([^,]+), map_location=([^)]+)\\)(?!.*weights_only)\",\n",
        "         r\"torch.load(\\1, map_location=\\2, weights_only=False)\"),\n",
        "        (r\"torch\\.load\\(cache_path\\)(?!.*weights_only)\",\n",
        "         r\"torch.load(cache_path, weights_only=False)\"),\n",
        "        (r\"torch\\.load\\(f, map_location=torch\\.device\\('cpu'\\)\\)(?!.*weights_only)\",\n",
        "         r\"torch.load(f, map_location=torch.device('cpu'), weights_only=False)\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in patterns:\n",
        "        content = re.sub(pattern, replacement, content)\n",
        "\n",
        "    if content != original:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "files_to_fix = [\n",
        "    'models/experimental.py',\n",
        "    'utils/datasets.py',\n",
        "    'utils/general.py'\n",
        "]\n",
        "\n",
        "for filepath in files_to_fix:\n",
        "    if fix_torch_load(filepath):\n",
        "        print(f\"Fixed: {filepath}\")\n",
        "    else:\n",
        "        print(f\"Skipped: {filepath}\")\n",
        "\n",
        "print(\"\\n YOLOv7 files patched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Vo5bbkru1g"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload your trained YOLOv7 weights (best.pt)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    weights_filename = list(uploaded.keys())[0]\n",
        "    YOLO_WEIGHTS = f'/content/yolov7/{weights_filename}'\n",
        "\n",
        "    # Move to yolov7 directory if not already there\n",
        "    if not os.path.exists(YOLO_WEIGHTS):\n",
        "        import shutil\n",
        "        shutil.move(weights_filename, YOLO_WEIGHTS)\n",
        "\n",
        "    print(f\"\\n Weights uploaded: {YOLO_WEIGHTS}\")\n",
        "else:\n",
        "    print(\" No weights uploaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjGgUOtTru1g"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/yolov7')\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# YOLOv7 imports\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import check_img_size, non_max_suppression, scale_coords\n",
        "from utils.torch_utils import select_device\n",
        "from utils.datasets import letterbox\n",
        "\n",
        "print(\" YOLOv7 modules imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbuITiWXru1g"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv7 model\n",
        "device = select_device('0')  # Use GPU\n",
        "yolo_model = attempt_load(YOLO_WEIGHTS, map_location=device)\n",
        "yolo_model.eval()\n",
        "\n",
        "# Get model stride and image size\n",
        "stride = int(yolo_model.stride.max())\n",
        "img_size = check_img_size(640, s=stride)\n",
        "\n",
        "# Get class names\n",
        "names = yolo_model.module.names if hasattr(yolo_model, 'module') else yolo_model.names\n",
        "\n",
        "print(f\"\\n YOLOv7 model loaded!\")\n",
        "print(f\"   Classes: {names}\")\n",
        "print(f\"   Image size: {img_size}\")\n",
        "print(f\"   Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WvdjDSLru1g"
      },
      "outputs": [],
      "source": [
        "# Load Florence-2 model\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "florence_model_id = \"microsoft/Florence-2-base\"\n",
        "\n",
        "print(\"Loading Florence-2 model\")\n",
        "florence_processor = AutoProcessor.from_pretrained(florence_model_id, trust_remote_code=True)\n",
        "florence_model = AutoModelForCausalLM.from_pretrained(\n",
        "    florence_model_id,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"eager\"\n",
        ").to(device)\n",
        "florence_model.eval()\n",
        "\n",
        "print(f\"\\n Florence-2 model loaded\")\n",
        "print(f\"   Model: {florence_model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJABW7u_ru1g"
      },
      "outputs": [],
      "source": [
        "def run_yolo_detection(image_path, conf_threshold=0.25, iou_threshold=0.45):\n",
        "    \"\"\"\n",
        "    Run YOLOv7 detection on an image.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        conf_threshold: Confidence threshold for detections\n",
        "        iou_threshold: IoU threshold for NMS\n",
        "\n",
        "    Returns:\n",
        "        original_image: Original image (BGR)\n",
        "        detections: List of [x1, y1, x2, y2, confidence, class_id]\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img0 = cv2.imread(image_path)  # BGR\n",
        "    if img0 is None:\n",
        "        raise ValueError(f\"Could not load image: {image_path}\")\n",
        "\n",
        "    # Preprocess\n",
        "    img = letterbox(img0, img_size, stride=stride)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, HWC to CHW\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.float() / 255.0\n",
        "\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        pred = yolo_model(img)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, conf_threshold, iou_threshold)\n",
        "\n",
        "    detections = []\n",
        "    for det in pred:\n",
        "        if len(det):\n",
        "            # Rescale boxes to original image size\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "            for *xyxy, conf, cls in det:\n",
        "                x1, y1, x2, y2 = [int(x) for x in xyxy]\n",
        "                detections.append({\n",
        "                    'bbox': [x1, y1, x2, y2],\n",
        "                    'confidence': float(conf),\n",
        "                    'class_id': int(cls),\n",
        "                    'class_name': names[int(cls)]\n",
        "                })\n",
        "\n",
        "    return img0, detections\n",
        "\n",
        "print(\" YOLOv7 detection function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2hKuABzru1h"
      },
      "outputs": [],
      "source": [
        "def run_florence_interpretation(image, task=\"<OCR>\"):\n",
        "    \"\"\"\n",
        "    Run Florence-2 interpretation on an image.\n",
        "    \"\"\"\n",
        "    # Convert to PIL if needed\n",
        "    if isinstance(image, np.ndarray):\n",
        "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    # Process with Florence-2\n",
        "    inputs = florence_processor(\n",
        "        text=task,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device, torch.float16)\n",
        "\n",
        "    # Generate with cache disabled\n",
        "    with torch.no_grad():\n",
        "        generated_ids = florence_model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3,\n",
        "            do_sample=False,\n",
        "            use_cache=False  # Add this line to fix the error\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = florence_processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=False\n",
        "    )[0]\n",
        "\n",
        "    # Post-process\n",
        "    result = florence_processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task=task,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\" Florence-2 interpretation function updated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_heoUoxru1h"
      },
      "outputs": [],
      "source": [
        "def process_image_pipeline(image_path, conf_threshold=0.25, padding=10):\n",
        "    \"\"\"\n",
        "    Full pipeline: YOLOv7 detection â†’ Florence-2 interpretation.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        conf_threshold: Confidence threshold for YOLO\n",
        "        padding: Extra pixels around detected region for cropping\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with all detection and interpretation results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {Path(image_path).name}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Step 1: YOLOv7 Detection\n",
        "    print(\"\\n Step 1: Running YOLOv7 detection...\")\n",
        "    original_img, detections = run_yolo_detection(image_path, conf_threshold)\n",
        "    print(f\"   Found {len(detections)} door plaque(s)\")\n",
        "\n",
        "    results = {\n",
        "        'image_path': image_path,\n",
        "        'original_image': original_img,\n",
        "        'detections': [],\n",
        "        'annotated_image': original_img.copy()\n",
        "    }\n",
        "\n",
        "    if len(detections) == 0:\n",
        "        print(\"   No plaques detected.\")\n",
        "        return results\n",
        "\n",
        "    # Step 2: Process each detection with Florence-2\n",
        "    print(\"\\n Step 2: Running Florence-2 interpretation...\")\n",
        "\n",
        "    h, w = original_img.shape[:2]\n",
        "\n",
        "    for i, det in enumerate(detections):\n",
        "        x1, y1, x2, y2 = det['bbox']\n",
        "\n",
        "        # Add padding and clip to image bounds\n",
        "        x1_pad = max(0, x1 - padding)\n",
        "        y1_pad = max(0, y1 - padding)\n",
        "        x2_pad = min(w, x2 + padding)\n",
        "        y2_pad = min(h, y2 + padding)\n",
        "\n",
        "        # Crop detected region\n",
        "        crop = original_img[y1_pad:y2_pad, x1_pad:x2_pad]\n",
        "\n",
        "        # Run Florence-2 OCR\n",
        "        print(f\"\\n   Plaque {i+1}:\")\n",
        "        print(f\"   - BBox: [{x1}, {y1}, {x2}, {y2}]\")\n",
        "        print(f\"   - Confidence: {det['confidence']:.2%}\")\n",
        "\n",
        "        # OCR\n",
        "        ocr_result = run_florence_interpretation(crop, \"<OCR>\")\n",
        "        ocr_text = ocr_result.get('<OCR>', '')\n",
        "        print(f\"   - OCR Text: {ocr_text}\")\n",
        "\n",
        "        # Caption (for additional context)\n",
        "        caption_result = run_florence_interpretation(crop, \"<CAPTION>\")\n",
        "        caption = caption_result.get('<CAPTION>', '')\n",
        "        print(f\"   - Caption: {caption}\")\n",
        "\n",
        "        # Store results\n",
        "        detection_result = {\n",
        "            'bbox': det['bbox'],\n",
        "            'confidence': det['confidence'],\n",
        "            'class_name': det['class_name'],\n",
        "            'crop': crop,\n",
        "            'ocr_text': ocr_text,\n",
        "            'caption': caption\n",
        "        }\n",
        "        results['detections'].append(detection_result)\n",
        "\n",
        "        # Draw on annotated image\n",
        "        cv2.rectangle(results['annotated_image'], (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text label\n",
        "        label = f\"{ocr_text[:30]}...\" if len(ocr_text) > 30 else ocr_text\n",
        "        label = label if label else f\"Plaque {i+1}\"\n",
        "\n",
        "        # Background for text\n",
        "        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "        cv2.rectangle(results['annotated_image'],\n",
        "                      (x1, y1 - th - 10), (x1 + tw + 10, y1),\n",
        "                      (0, 255, 0), -1)\n",
        "        cv2.putText(results['annotated_image'], label,\n",
        "                    (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.6, (0, 0, 0), 2)\n",
        "\n",
        "    print(f\"\\n Processing complete\")\n",
        "    return results\n",
        "\n",
        "print(\" Full pipeline function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqdgFLI9ru1h"
      },
      "outputs": [],
      "source": [
        "def visualize_results(results, figsize=(16, 10)):\n",
        "    \"\"\"\n",
        "    Visualize pipeline results.\n",
        "    \"\"\"\n",
        "    detections = results['detections']\n",
        "    n_detections = len(detections)\n",
        "\n",
        "    if n_detections == 0:\n",
        "        # Just show original image\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(cv2.cvtColor(results['original_image'], cv2.COLOR_BGR2RGB))\n",
        "        plt.title('No plaques detected')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    # Main annotated image\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.imshow(cv2.cvtColor(results['annotated_image'], cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(f'Detected Plaques ({n_detections} found)', fontsize=14)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Cropped regions with OCR results\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title('OCR Results', fontsize=14)\n",
        "\n",
        "    # Create grid of crops\n",
        "    n_cols = min(2, n_detections)\n",
        "    n_rows = (n_detections + n_cols - 1) // n_cols\n",
        "\n",
        "    for i, det in enumerate(detections):\n",
        "        # Create subplot for each crop\n",
        "        ax_crop = fig.add_subplot(n_rows, 2 + n_cols, 3 + i)\n",
        "        ax_crop.imshow(cv2.cvtColor(det['crop'], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        ocr_text = det['ocr_text'] if det['ocr_text'] else '(no text)'\n",
        "        # Truncate long text\n",
        "        if len(ocr_text) > 50:\n",
        "            ocr_text = ocr_text[:47] + '...'\n",
        "\n",
        "        ax_crop.set_title(f\"Plaque {i+1}\\n{ocr_text}\", fontsize=10, wrap=True)\n",
        "        ax_crop.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, det in enumerate(detections):\n",
        "        print(f\"\\n Plaque {i+1}:\")\n",
        "        print(f\"   Confidence: {det['confidence']:.2%}\")\n",
        "        print(f\"   Bounding Box: {det['bbox']}\")\n",
        "        print(f\"   OCR Text: {det['ocr_text']}\")\n",
        "        print(f\"   Caption: {det['caption']}\")\n",
        "\n",
        "print(\" Visualization function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_pipeline_timed(image_path, conf_threshold=0.25, padding=10):\n",
        "    \"\"\"\n",
        "    Full pipeline with detailed timing.\n",
        "    \"\"\"\n",
        "    import time\n",
        "    timings = {}\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {Path(image_path).name}\")\n",
        "    print('='*60)\n",
        "\n",
        "    print(\"\\n Step 1: Running YOLOv7 detection...\")\n",
        "    t0 = time.time()\n",
        "    original_img, detections = run_yolo_detection(image_path, conf_threshold)\n",
        "    timings['yolo'] = time.time() - t0\n",
        "    print(f\"   Found {len(detections)} door plaque(s) ({timings['yolo']:.3f}s)\")\n",
        "\n",
        "    results = {\n",
        "        'image_path': image_path,\n",
        "        'original_image': original_img,\n",
        "        'detections': [],\n",
        "        'annotated_image': original_img.copy(),\n",
        "        'timings': timings\n",
        "    }\n",
        "\n",
        "    if len(detections) == 0:\n",
        "        print(\"   No plaques detected.\")\n",
        "        timings['florence_ocr'] = 0\n",
        "        timings['florence_caption'] = 0\n",
        "        return results\n",
        "\n",
        "    print(\"\\n Step 2: Running Florence-2 interpretation...\")\n",
        "\n",
        "    h, w = original_img.shape[:2]\n",
        "    timings['florence_ocr'] = 0\n",
        "    timings['florence_caption'] = 0\n",
        "\n",
        "    for i, det in enumerate(detections):\n",
        "        x1, y1, x2, y2 = det['bbox']\n",
        "\n",
        "        # Add padding and clip to image bounds\n",
        "        x1_pad = max(0, x1 - padding)\n",
        "        y1_pad = max(0, y1 - padding)\n",
        "        x2_pad = min(w, x2 + padding)\n",
        "        y2_pad = min(h, y2 + padding)\n",
        "\n",
        "        # Crop detected region\n",
        "        crop = original_img[y1_pad:y2_pad, x1_pad:x2_pad]\n",
        "\n",
        "        print(f\"\\n   Plaque {i+1}:\")\n",
        "        print(f\"   - BBox: [{x1}, {y1}, {x2}, {y2}]\")\n",
        "        print(f\"   - Confidence: {det['confidence']:.2%}\")\n",
        "\n",
        "        # OCR with timing\n",
        "        t0 = time.time()\n",
        "        ocr_result = run_florence_interpretation(crop, \"<OCR>\")\n",
        "        timings['florence_ocr'] += time.time() - t0\n",
        "        ocr_text = ocr_result.get('<OCR>', '')\n",
        "        print(f\"   - OCR Text: {ocr_text}\")\n",
        "\n",
        "        # Caption with timing\n",
        "        t0 = time.time()\n",
        "        caption_result = run_florence_interpretation(crop, \"<CAPTION>\")\n",
        "        timings['florence_caption'] += time.time() - t0\n",
        "        caption = caption_result.get('<CAPTION>', '')\n",
        "        print(f\"   - Caption: {caption}\")\n",
        "\n",
        "        # Store results\n",
        "        detection_result = {\n",
        "            'bbox': det['bbox'],\n",
        "            'confidence': det['confidence'],\n",
        "            'class_name': det['class_name'],\n",
        "            'crop': crop,\n",
        "            'ocr_text': ocr_text,\n",
        "            'caption': caption\n",
        "        }\n",
        "        results['detections'].append(detection_result)\n",
        "\n",
        "        # Draw on annotated image\n",
        "        cv2.rectangle(results['annotated_image'], (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        label = f\"{ocr_text[:30]}...\" if len(ocr_text) > 30 else ocr_text\n",
        "        label = label if label else f\"Plaque {i+1}\"\n",
        "\n",
        "        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "        cv2.rectangle(results['annotated_image'],\n",
        "                      (x1, y1 - th - 10), (x1 + tw + 10, y1),\n",
        "                      (0, 255, 0), -1)\n",
        "        cv2.putText(results['annotated_image'], label,\n",
        "                    (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.6, (0, 0, 0), 2)\n",
        "\n",
        "    # Print timing summary for this image\n",
        "    total = timings['yolo'] + timings['florence_ocr'] + timings['florence_caption']\n",
        "    print(f\"\\n    Timing: YOLO={timings['yolo']:.2f}s | OCR={timings['florence_ocr']:.2f}s | Caption={timings['florence_caption']:.2f}s | Total={total:.2f}s\")\n",
        "\n",
        "    results['timings'] = timings\n",
        "    return results\n",
        "\n",
        "print(\" Timed pipeline function defined\")"
      ],
      "metadata": {
        "id": "yYOgnuf_HAqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jZaUStlru1h"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Upload your dataset ZIP file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n Extracting {zip_filename}...\")\n",
        "\n",
        "    # Extract\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/dataset')\n",
        "\n",
        "    # Find actual dataset path\n",
        "    dataset_contents = os.listdir('/content/dataset')\n",
        "    if len(dataset_contents) == 1 and os.path.isdir(f'/content/dataset/{dataset_contents[0]}'):\n",
        "        DATASET_PATH = f'/content/dataset/{dataset_contents[0]}'\n",
        "    else:\n",
        "        DATASET_PATH = '/content/dataset'\n",
        "\n",
        "    print(f\" Dataset extracted to: {DATASET_PATH}\")\n",
        "\n",
        "    # Set image paths\n",
        "    TRAIN_IMAGES = os.path.join(DATASET_PATH, 'train/images')\n",
        "    VALID_IMAGES = os.path.join(DATASET_PATH, 'valid/images')\n",
        "    TEST_IMAGES = os.path.join(DATASET_PATH, 'test/images')\n",
        "\n",
        "    # Get test images\n",
        "    import glob\n",
        "    test_images = glob.glob(f\"{TEST_IMAGES}/*.jpg\") + glob.glob(f\"{TEST_IMAGES}/*.png\")\n",
        "\n",
        "    print(f\"\\n Found {len(test_images)} test images\")\n",
        "    print(f\"First image: {test_images[0] if test_images else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lzp6PbAru1h"
      },
      "outputs": [],
      "source": [
        "# Run pipeline on all uploaded images\n",
        "all_results = []\n",
        "\n",
        "for image_path in test_images:\n",
        "    results = process_image_pipeline(\n",
        "        image_path,\n",
        "        conf_threshold=0.25,  # Adjust as needed\n",
        "        padding=15             # Extra pixels around crops\n",
        "    )\n",
        "    all_results.append(results)\n",
        "    visualize_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for image_path in tqdm(test_images, desc=\"Processing\"):\n",
        "    results = process_image_pipeline_timed(\n",
        "        image_path,\n",
        "        conf_threshold=0.25,\n",
        "        padding=15\n",
        "    )\n",
        "    all_results.append(results)\n",
        "\n",
        "# Aggregate timing stats\n",
        "yolo_times = [r['timings']['yolo'] for r in all_results]\n",
        "ocr_times = [r['timings']['florence_ocr'] for r in all_results]\n",
        "caption_times = [r['timings']['florence_caption'] for r in all_results]\n",
        "total_times = [sum(r['timings'].values()) for r in all_results]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  TIMING SUMMARY ({len(all_results)} images)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"{'Component':<20} {'Total':>10} {'Average':>10} {'Min':>10} {'Max':>10}\")\n",
        "print(f\"{'-'*60}\")\n",
        "print(f\"{'YOLOv7':<20} {sum(yolo_times):>9.2f}s {sum(yolo_times)/len(yolo_times):>9.2f}s {min(yolo_times):>9.2f}s {max(yolo_times):>9.2f}s\")\n",
        "print(f\"{'Florence-2 OCR':<20} {sum(ocr_times):>9.2f}s {sum(ocr_times)/len(ocr_times):>9.2f}s {min(ocr_times):>9.2f}s {max(ocr_times):>9.2f}s\")\n",
        "print(f\"{'Florence-2 Caption':<20} {sum(caption_times):>9.2f}s {sum(caption_times)/len(caption_times):>9.2f}s {min(caption_times):>9.2f}s {max(caption_times):>9.2f}s\")\n",
        "print(f\"{'-'*60}\")\n",
        "print(f\"{'TOTAL':<20} {sum(total_times):>9.2f}s {sum(total_times)/len(total_times):>9.2f}s {min(total_times):>9.2f}s {max(total_times):>9.2f}s\")\n",
        "print(f\"\\n Throughput: {len(all_results)/sum(total_times):.2f} images/second\")"
      ],
      "metadata": {
        "id": "G7QCJveNHeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh6zg9E5ru1h"
      },
      "outputs": [],
      "source": [
        "def process_folder(folder_path, output_dir=None, conf_threshold=0.25):\n",
        "    \"\"\"\n",
        "    Process all images in a folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to folder containing images\n",
        "        output_dir: Optional output directory for annotated images\n",
        "        conf_threshold: Confidence threshold for detection\n",
        "\n",
        "    Returns:\n",
        "        results_df: DataFrame with all results\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    from pathlib import Path\n",
        "\n",
        "    folder = Path(folder_path)\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
        "\n",
        "    images = [f for f in folder.iterdir()\n",
        "              if f.suffix.lower() in image_extensions]\n",
        "\n",
        "    print(f\"Found {len(images)} images in {folder_path}\")\n",
        "\n",
        "    if output_dir:\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    all_records = []\n",
        "\n",
        "    for img_path in images:\n",
        "        try:\n",
        "            results = process_image_pipeline(\n",
        "                str(img_path),\n",
        "                conf_threshold=conf_threshold\n",
        "            )\n",
        "\n",
        "            # Save annotated image if output_dir specified\n",
        "            if output_dir:\n",
        "                out_path = output_path / f\"annotated_{img_path.name}\"\n",
        "                cv2.imwrite(str(out_path), results['annotated_image'])\n",
        "\n",
        "            # Record results\n",
        "            for i, det in enumerate(results['detections']):\n",
        "                all_records.append({\n",
        "                    'image': img_path.name,\n",
        "                    'plaque_id': i + 1,\n",
        "                    'confidence': det['confidence'],\n",
        "                    'bbox': str(det['bbox']),\n",
        "                    'ocr_text': det['ocr_text'],\n",
        "                    'caption': det['caption']\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path.name}: {e}\")\n",
        "\n",
        "    results_df = pd.DataFrame(all_records)\n",
        "\n",
        "    print(f\"\\n Processed {len(images)} images, found {len(all_records)} plaques\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "print(\" Batch processing function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOYSBqM8ru1h"
      },
      "outputs": [],
      "source": [
        "if len(all_results) > 0 and len(all_results[0]['detections']) > 0:\n",
        "    sample_crop = all_results[0]['detections'][0]['crop']\n",
        "\n",
        "    print(\"Testing different Florence-2 tasks on a sample crop:\\n\")\n",
        "\n",
        "    tasks = [\n",
        "        (\"<OCR>\", \"OCR (text extraction)\"),\n",
        "        (\"<OCR_WITH_REGION>\", \"OCR with regions\"),\n",
        "        (\"<CAPTION>\", \"Caption\"),\n",
        "        (\"<DETAILED_CAPTION>\", \"Detailed Caption\"),\n",
        "        (\"<MORE_DETAILED_CAPTION>\", \"Very Detailed Caption\"),\n",
        "    ]\n",
        "\n",
        "    for task_prompt, task_name in tasks:\n",
        "        try:\n",
        "            result = run_florence_interpretation(sample_crop, task_prompt)\n",
        "            output = result.get(task_prompt, result)\n",
        "            print(f\" {task_name}:\")\n",
        "            print(f\"   {output}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\" {task_name}: {e}\\n\")\n",
        "else:\n",
        "    print(\"No detections available for testing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6DTPdXeru1h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def export_results(all_results, output_prefix='plaque_results'):\n",
        "    \"\"\"\n",
        "    Export results to CSV and JSON files.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for result in all_results:\n",
        "        image_name = Path(result['image_path']).name\n",
        "\n",
        "        for i, det in enumerate(result['detections']):\n",
        "            records.append({\n",
        "                'image': image_name,\n",
        "                'plaque_id': i + 1,\n",
        "                'confidence': det['confidence'],\n",
        "                'x1': det['bbox'][0],\n",
        "                'y1': det['bbox'][1],\n",
        "                'x2': det['bbox'][2],\n",
        "                'y2': det['bbox'][3],\n",
        "                'ocr_text': det['ocr_text'],\n",
        "                'caption': det['caption']\n",
        "            })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Save CSV\n",
        "    csv_path = f'{output_prefix}.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\" Saved: {csv_path}\")\n",
        "\n",
        "    # Save JSON\n",
        "    json_path = f'{output_prefix}.json'\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(records, f, indent=2)\n",
        "    print(f\" Saved: {json_path}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Export\n",
        "if all_results:\n",
        "    results_df = export_results(all_results)\n",
        "    display(results_df)\n",
        "\n",
        "    # Download files\n",
        "    from google.colab import files\n",
        "    files.download('plaque_results.csv')\n",
        "    files.download('plaque_results.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LsmPCB6ru1h"
      },
      "outputs": [],
      "source": [
        "%%writefile inference_pipeline.py\n",
        "\"\"\"\n",
        "Door Plaque Detection & Interpretation Pipeline\n",
        "YOLOv7 + Florence-2\n",
        "\n",
        "Usage:\n",
        "    python inference_pipeline.py --image path/to/image.jpg --weights path/to/best.pt\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "def load_models(yolo_weights, device='cuda'):\n",
        "    \"\"\"Load YOLOv7 and Florence-2 models.\"\"\"\n",
        "    # Add YOLOv7 to path\n",
        "    yolov7_path = Path(yolo_weights).parent\n",
        "    sys.path.insert(0, str(yolov7_path))\n",
        "\n",
        "    from models.experimental import attempt_load\n",
        "    from utils.general import check_img_size\n",
        "    from utils.torch_utils import select_device\n",
        "\n",
        "    # Load YOLOv7\n",
        "    device = select_device(device)\n",
        "    yolo = attempt_load(yolo_weights, map_location=device)\n",
        "    yolo.eval()\n",
        "    stride = int(yolo.stride.max())\n",
        "    img_size = check_img_size(640, s=stride)\n",
        "    names = yolo.module.names if hasattr(yolo, 'module') else yolo.names\n",
        "\n",
        "    # Load Florence-2\n",
        "    from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "    florence_processor = AutoProcessor.from_pretrained(\n",
        "        \"microsoft/Florence-2-base\", trust_remote_code=True\n",
        "    )\n",
        "    florence = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/Florence-2-base\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "    florence.eval()\n",
        "\n",
        "    return {\n",
        "        'yolo': yolo,\n",
        "        'florence': florence,\n",
        "        'florence_processor': florence_processor,\n",
        "        'device': device,\n",
        "        'stride': stride,\n",
        "        'img_size': img_size,\n",
        "        'names': names\n",
        "    }\n",
        "\n",
        "def process_image(image_path, models, conf_threshold=0.25):\n",
        "    \"\"\"Process a single image through the pipeline.\"\"\"\n",
        "    from utils.general import non_max_suppression, scale_coords\n",
        "    from utils.datasets import letterbox\n",
        "\n",
        "    # Load image\n",
        "    img0 = cv2.imread(image_path)\n",
        "\n",
        "    # YOLOv7 detection\n",
        "    img = letterbox(img0, models['img_size'], stride=models['stride'])[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(models['device']).float() / 255.0\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = models['yolo'](img)[0]\n",
        "    pred = non_max_suppression(pred, conf_threshold, 0.45)\n",
        "\n",
        "    results = []\n",
        "    for det in pred:\n",
        "        if len(det):\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "            for *xyxy, conf, cls in det:\n",
        "                x1, y1, x2, y2 = [int(x) for x in xyxy]\n",
        "                crop = img0[y1:y2, x1:x2]\n",
        "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "                crop_pil = Image.fromarray(crop_rgb)\n",
        "\n",
        "                # Florence-2 OCR\n",
        "                inputs = models['florence_processor'](\n",
        "                    text=\"<OCR>\", images=crop_pil, return_tensors=\"pt\"\n",
        "                ).to(models['device'], torch.float16)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    generated = models['florence'].generate(\n",
        "                        input_ids=inputs[\"input_ids\"],\n",
        "                        pixel_values=inputs[\"pixel_values\"],\n",
        "                        max_new_tokens=512\n",
        "                    )\n",
        "\n",
        "                text = models['florence_processor'].batch_decode(\n",
        "                    generated, skip_special_tokens=False\n",
        "                )[0]\n",
        "                ocr_result = models['florence_processor'].post_process_generation(\n",
        "                    text, task=\"<OCR>\", image_size=(crop_pil.width, crop_pil.height)\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    'bbox': [x1, y1, x2, y2],\n",
        "                    'confidence': float(conf),\n",
        "                    'ocr_text': ocr_result.get('<OCR>', '')\n",
        "                })\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--image', required=True, help='Path to input image')\n",
        "    parser.add_argument('--weights', required=True, help='Path to YOLOv7 weights')\n",
        "    parser.add_argument('--conf', type=float, default=0.25, help='Confidence threshold')\n",
        "    parser.add_argument('--device', default='cuda', help='Device (cuda/cpu)')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Loading models...\")\n",
        "    models = load_models(args.weights, args.device)\n",
        "\n",
        "    print(f\"Processing {args.image}...\")\n",
        "    results = process_image(args.image, models, args.conf)\n",
        "\n",
        "    print(f\"\\nFound {len(results)} plaque(s):\")\n",
        "    for i, r in enumerate(results):\n",
        "        print(f\"  {i+1}. [{r['confidence']:.2%}] {r['ocr_text']}\")\n",
        "\n",
        "print(\"\\n Standalone script saved as 'inference_pipeline.py'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}